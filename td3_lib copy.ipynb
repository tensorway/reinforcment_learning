{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit41770f51cb494085b126429b02db281f",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import gym\n",
    "import myrl\n",
    "from myrl.buffers import ReplayBuffer, PrioritizedReplayBuffer\n",
    "from myrl.environments import Envs\n",
    "from myrl.value_functions import ValueFunctionMLP, polyak\n",
    "from myrl.policies import LinearPolicy, RandomPolicy\n",
    "from myrl.visualizer import showit\n",
    "from myrl.utils import normal_noise, ExperimentWriter\n",
    "import torch\n",
    "import copy\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "z = 0\n",
    "envname = 'Pendulum-v0'\n",
    "env = gym.make(envname)\n",
    "envs = Envs(envname, 3)\n",
    "random_policy = RandomPolicy(env).act\n",
    "obsdim = env.observation_space.shape[0]\n",
    "adim = env.action_space.shape[0]\n",
    "wll = ExperimentWriter('tb/pentd3')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() or 0 else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rbuff = ReplayBuffer(max_len=100000)\n",
    "\n",
    "maxer = LinearPolicy(obsdim, 32, adim)\n",
    "q1 = ValueFunctionMLP(obsdim+adim, 64, 32, 1)\n",
    "q2 = ValueFunctionMLP(obsdim+adim, 64, 32, 1)\n",
    "tq1 = copy.deepcopy(q1)\n",
    "tq2 = copy.deepcopy(q2)\n",
    "opt = torch.optim.Adam(list(q1.parameters())+list(q2.parameters()), lr=1e-3)\n",
    "mopt = torch.optim.Adam(maxer.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chck(pi):\n",
    "    import numpy as np \n",
    "    nes = np.expand_dims(env.observation_space.sample(), axis=0)\n",
    "    a, (d, sm, h)=pi.act(nes)\n",
    "    return d\n",
    "def clip_grad_norm_(module, max_grad_norm):\n",
    "    nn.utils.clip_grad_norm_([p for g in module.param_groups for p in g[\"params\"]], max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 133.97738647460938 -6.336296081542969 8.408854484558105 100000 -1.0595449209213257 -6.513232707977295 tb/pentd36\n100 135.8064727783203 -6.703187942504883 34.17329788208008 100000 -1.3588658571243286 -7.248016834259033 tb/pentd36\n120 133.97274780273438 -7.099801540374756 37.11856460571289 100000 -1.2055333852767944 -7.416893482208252 tb/pentd36\n140 140.18563842773438 -7.476479530334473 3.5062918663024902 100000 -1.0993207693099976 -7.140685081481934 tb/pentd36\n160 136.23696899414062 -7.02285099029541 76.46674346923828 100000 0.4909179210662842 -7.197425365447998 tb/pentd36\n180 138.33795166015625 -6.810129642486572 69.14186096191406 100000 -1.690279483795166 -6.618500232696533 tb/pentd36\n200 136.7490234375 -6.778636932373047 3.575767993927002 100000 -0.07536043971776962 -7.258176326751709 tb/pentd36\n198 /2000220 140.67257690429688 -6.941531181335449 1.0943195819854736 100000 0.14448195695877075 -7.555851459503174 tb/pentd36\n240 141.18626403808594 -7.236208915710449 0.9635403156280518 100000 0.24728697538375854 -7.4958648681640625 tb/pentd36\n260 145.880126953125 -7.7571868896484375 11.273122787475586 100000 0.14415393769741058 -7.426426410675049 tb/pentd36\n280 141.72796630859375 -6.7030181884765625 1.491920828819275 100000 0.10714729130268097 -7.6704559326171875 tb/pentd36\n300 140.75010681152344 -6.929055690765381 6.671204566955566 100000 -1.7525700330734253 -7.589925289154053 tb/pentd36\n320 142.74879455566406 -7.088834762573242 18.399879455566406 100000 0.6230205297470093 -6.2860107421875 tb/pentd36\n340 141.46914672851562 -7.103869438171387 37.577518463134766 100000 0.5230331420898438 -6.701927661895752 tb/pentd36\n360 143.23802185058594 -7.667105674743652 1.9844244718551636 100000 -0.6266881823539734 -6.527245998382568 tb/pentd36\n380 142.8514862060547 -7.653112411499023 9.896310806274414 100000 0.06559408456087112 -6.460855484008789 tb/pentd36\n400 142.39346313476562 -6.878233909606934 50.582271575927734 100000 0.1252814680337906 -7.716063499450684 tb/pentd36\n198 /2000420 143.75103759765625 -7.131478309631348 105.38033294677734 100000 -0.559082567691803 -6.166591644287109 tb/pentd36\n440 142.28334045410156 -6.880000114440918 11.424657821655273 100000 -0.2260400354862213 -7.394298553466797 tb/pentd36\n460 139.4830322265625 -6.75540018081665 9.263309478759766 100000 -1.5161170959472656 -6.404733657836914 tb/pentd36\n480 140.71865844726562 -6.154532432556152 11.665624618530273 100000 0.5511624217033386 -7.310873985290527 tb/pentd36\n500 144.46766662597656 -7.004690170288086 31.411739349365234 100000 0.6540107727050781 -6.4470109939575195 tb/pentd36\n520 142.11708068847656 -6.90043830871582 18.468242645263672 100000 0.5589005947113037 -7.533607482910156 tb/pentd36\n540 142.20616149902344 -7.195188045501709 129.80596923828125 100000 0.5099753141403198 -7.169737339019775 tb/pentd36\n560 142.91812133789062 -7.32431697845459 153.34950256347656 100000 -0.2783585786819458 -7.1506853103637695 tb/pentd36\n580 141.84625244140625 -7.112556457519531 36.53789138793945 100000 -0.562282383441925 -7.460414409637451 tb/pentd36\n600 139.48043823242188 -6.987616539001465 1.9661346673965454 100000 -1.6561846733093262 -7.27479887008667 tb/pentd36\n198 /2000620 143.67213439941406 -7.4352312088012695 1.5547544956207275 100000 0.2658956050872803 -7.4371442794799805 tb/pentd36\n640 143.3181915283203 -6.579509258270264 48.960262298583984 100000 0.04701749235391617 -7.678267478942871 tb/pentd36\n660 142.065185546875 -6.61058235168457 17.92169189453125 100000 0.25485315918922424 -6.449094295501709 tb/pentd36\n680 139.11697387695312 -6.400783538818359 181.2527313232422 100000 -0.94732666015625 -7.2426676750183105 tb/pentd36\n700 138.450927734375 -7.3423171043396 9.70504093170166 100000 -1.5392735004425049 -6.012843608856201 tb/pentd36\n720 141.1916961669922 -6.812567234039307 55.21599578857422 100000 0.3190680742263794 -7.665645122528076 tb/pentd36\n740 141.90591430664062 -6.6081647872924805 4.099358558654785 100000 -1.6448931694030762 -7.496989727020264 tb/pentd36\n760 146.71340942382812 -7.727020263671875 34.62205505371094 100000 0.7136127352714539 -7.8079833984375 tb/pentd36\n780 140.19708251953125 -6.5540971755981445 5.5732035636901855 100000 0.6355037689208984 -6.421759605407715 tb/pentd36\n800 138.89157104492188 -7.232964992523193 17.275856018066406 100000 -1.2339520454406738 -7.557030200958252 tb/pentd36\n198 /2000820 143.35565185546875 -7.225341320037842 68.82324981689453 100000 -0.18521377444267273 -6.439212799072266 tb/pentd36\n840 144.30039978027344 -7.305575847625732 8.996339797973633 100000 -0.0003587305545806885 -6.580575942993164 tb/pentd36\n860 143.3314971923828 -7.1045331954956055 14.516532897949219 100000 -1.3734208345413208 -6.327349662780762 tb/pentd36\n880 143.35015869140625 -7.095439910888672 46.18231201171875 100000 -0.7792200446128845 -7.167921543121338 tb/pentd36\n900 141.7130126953125 -6.692028045654297 85.79553985595703 100000 -0.23933756351470947 -6.292815208435059 tb/pentd36\n920 140.02536010742188 -7.027853012084961 65.98516845703125 100000 -0.5201935768127441 -7.551917552947998 tb/pentd36\n940 140.85255432128906 -6.838803768157959 0.8974173069000244 100000 -0.9934122562408447 -7.304412364959717 tb/pentd36\n960 145.26025390625 -6.987151145935059 44.6745491027832 100000 -0.7283392548561096 -6.320080757141113 tb/pentd36\n980 139.58091735839844 -6.785521984100342 7.434385299682617 100000 -1.74481999874115 -7.3649821281433105 tb/pentd36\n1000 140.56686401367188 -6.8786773681640625 44.70254898071289 100000 -0.9289764165878296 -7.202932357788086 tb/pentd36\n198 /20001020 142.579345703125 -7.5905375480651855 28.549232482910156 100000 0.25826358795166016 -7.644578456878662 tb/pentd36\n1040 140.59471130371094 -7.38149881362915 6.015605449676514 100000 -1.0343095064163208 -6.595673084259033 tb/pentd36\n1060 139.47415161132812 -7.356076240539551 52.891937255859375 100000 -1.0013787746429443 -6.507437705993652 tb/pentd36\n1080 141.89581298828125 -6.8083038330078125 26.55341339111328 100000 -1.4298573732376099 -7.398213863372803 tb/pentd36\n1100 145.5194854736328 -7.025876522064209 5.662219047546387 100000 -0.3772358000278473 -7.216514587402344 tb/pentd36\n1120 144.69692993164062 -7.607905864715576 113.28408813476562 100000 0.5961542129516602 -6.755297660827637 tb/pentd36\n1140 143.1155242919922 -7.277024269104004 1.2181665897369385 100000 -1.0302594900131226 -6.6517744064331055 tb/pentd36\n1160 143.64889526367188 -7.756018161773682 114.23748779296875 100000 0.03732787072658539 -7.463422775268555 tb/pentd36\n1180 138.2227020263672 -6.476509094238281 3.368936777114868 100000 0.17005588114261627 -7.261495590209961 tb/pentd36\n1200 143.9140167236328 -7.370112895965576 6.303648471832275 100000 -1.3714972734451294 -7.363957405090332 tb/pentd36\n198 /20001220 143.6122589111328 -6.574872016906738 33.73057556152344 100000 -1.777716875076294 -7.5701904296875 tb/pentd36\n1240 142.55001831054688 -6.62129020690918 26.296829223632812 100000 0.1497853547334671 -6.526325225830078 tb/pentd36\n1260 142.95619201660156 -7.187667369842529 63.790828704833984 100000 0.1974378377199173 -7.451685428619385 tb/pentd36\n1280 138.06141662597656 -6.771128177642822 5.170601844787598 100000 -1.657598853111267 -6.663550853729248 tb/pentd36\n1300 140.48446655273438 -7.037315368652344 19.84722137451172 100000 0.2968389391899109 -6.645607948303223 tb/pentd36\n1320 137.36590576171875 -7.017496585845947 0.8304691910743713 100000 -1.782175064086914 -6.641594886779785 tb/pentd36\n1340 140.06369018554688 -7.1940016746521 6.721075057983398 100000 -1.0944782495498657 -7.3034820556640625 tb/pentd36\n1360 140.24325561523438 -7.498072624206543 87.81396484375 100000 -0.36177605390548706 -6.577205181121826 tb/pentd36\n1380 145.01785278320312 -7.034269332885742 50.816009521484375 100000 0.3159688413143158 -6.628130912780762 tb/pentd36\n1400 146.04129028320312 -7.4987993240356445 14.832468032836914 100000 0.5884288549423218 -7.1695427894592285 tb/pentd36\n198 /20001420 140.68963623046875 -6.689651012420654 4.174062252044678 100000 -1.5865017175674438 -5.90753698348999 tb/pentd36\n1440 143.00181579589844 -6.697392463684082 0.8935110569000244 100000 -0.5877722501754761 -6.395670413970947 tb/pentd36\n1460 143.64962768554688 -7.235540866851807 34.32168960571289 100000 -1.0345300436019897 -7.067220211029053 tb/pentd36\n1480 142.04161071777344 -6.559757232666016 82.93211364746094 100000 -0.3799891769886017 -6.60735559463501 tb/pentd36\n1500 143.6268768310547 -7.090860366821289 2.1883161067962646 100000 -1.0686200857162476 -6.558219909667969 tb/pentd36\n1520 140.96690368652344 -7.008323669433594 137.58758544921875 100000 -1.1412628889083862 -4.843400001525879 tb/pentd36\n1540 140.50384521484375 -7.347466468811035 6.627922534942627 100000 0.02669190801680088 -7.712013244628906 tb/pentd36\n1560 142.76174926757812 -6.343912124633789 5.755728244781494 100000 -0.10950517654418945 -7.5862717628479 tb/pentd36\n1580 144.54547119140625 -7.049839973449707 36.0993766784668 100000 -0.6253921985626221 -7.386974334716797 tb/pentd36\n1600 149.38522338867188 -6.880566596984863 53.65801239013672 100000 -0.9797593951225281 -7.512851715087891 tb/pentd36\n198 /20001620 145.5120849609375 -7.86842155456543 2.4016921520233154 100000 -0.14167118072509766 -7.1916351318359375 tb/pentd36\n1640 143.55560302734375 -6.9665937423706055 1.505841851234436 100000 0.3818904459476471 -6.555996894836426 tb/pentd36\n1660 144.84120178222656 -6.942470550537109 55.06486511230469 100000 -0.43145328760147095 -6.760542392730713 tb/pentd36\n1680 144.96810913085938 -7.545191764831543 19.2966251373291 100000 0.0476570799946785 -7.075629234313965 tb/pentd36\n1700 138.79515075683594 -7.005499839782715 198.3309783935547 100000 -0.5653301477432251 -6.572786808013916 tb/pentd36\n1720 142.81301879882812 -6.726889133453369 33.823638916015625 100000 -0.3519171178340912 -7.544981479644775 tb/pentd36\n1740 141.7705841064453 -6.697303771972656 8.119524955749512 100000 0.06350378692150116 -7.343539237976074 tb/pentd36\n1760 141.4358673095703 -7.155389785766602 17.478055953979492 100000 -0.608342707157135 -7.955282211303711 tb/pentd36\n1780 139.4869384765625 -6.873318672180176 3.7314324378967285 100000 -0.04695351421833038 -6.642094612121582 tb/pentd36\n1800 139.80142211914062 -7.132404327392578 51.25258255004883 100000 -0.9209436774253845 -7.585694313049316 tb/pentd36\n198 /20001820 143.38072204589844 -6.544112205505371 91.3917007446289 100000 0.23642286658287048 -5.80722188949585 tb/pentd36\n1840 142.88479614257812 -7.540782928466797 32.5838623046875 100000 0.03850330039858818 -7.307110786437988 tb/pentd36\n1860 142.173583984375 -7.193417549133301 17.492008209228516 100000 -1.535695195198059 -7.182193279266357 tb/pentd36\n1880 142.98049926757812 -7.242259979248047 9.544815063476562 100000 0.08024206757545471 -7.2774505615234375 tb/pentd36\n1900 142.89047241210938 -7.456267833709717 6.921430587768555 100000 -1.7994422912597656 -6.227920055389404 tb/pentd36\n1920 144.41378784179688 -7.289007186889648 5.6141862869262695 100000 0.2639923691749573 -7.422101020812988 tb/pentd36\n1940 142.45375061035156 -7.440142631530762 10.06480884552002 100000 -0.975937008857727 -7.4488019943237305 tb/pentd36\n1960 143.2002716064453 -6.7961745262146 9.974021911621094 100000 -1.1176446676254272 -7.3900675773620605 tb/pentd36\n1980 140.05267333984375 -7.081133842468262 33.35859298706055 100000 -0.5507879853248596 -7.041200160980225 tb/pentd36\n2000 141.77537536621094 -7.156879425048828 0.34720170497894287 100000 -0.9790393710136414 -7.050740718841553 tb/pentd36\n198 /20002020 140.94534301757812 -6.685724258422852 2.3849899768829346 100000 -0.22817640006542206 -7.1668009757995605 tb/pentd36\n2040 140.5359344482422 -6.713204383850098 35.67324447631836 100000 -1.0022060871124268 -7.1905517578125 tb/pentd36\n2060 141.158203125 -7.65962028503418 9.832655906677246 100000 -0.9319095015525818 -6.613754749298096 tb/pentd36\n2080 142.364501953125 -7.269200325012207 40.98904800415039 100000 0.6315244436264038 -7.316386699676514 tb/pentd36\n2100 141.55142211914062 -7.064175605773926 9.013287544250488 100000 -1.5547778606414795 -6.583930492401123 tb/pentd36\n2120 144.6765594482422 -6.757401466369629 38.35826873779297 100000 -1.525053858757019 -7.474534511566162 tb/pentd36\n2140 140.14022827148438 -7.303410053253174 64.13221740722656 100000 -1.7398689985275269 -6.761892795562744 tb/pentd36\n2160 141.76210021972656 -6.721970558166504 38.39545822143555 100000 0.1277560293674469 -5.78481388092041 tb/pentd36\n2180 137.2559356689453 -7.384707450866699 3.061286211013794 100000 -0.0541251115500927 -6.5061798095703125 tb/pentd36\n2200 139.59475708007812 -7.012063026428223 21.975727081298828 100000 -1.543547511100769 -7.423222064971924 tb/pentd36\n198 /20002220 135.69529724121094 -7.304820537567139 53.54916763305664 100000 -0.14835990965366364 -7.416835308074951 tb/pentd36\n2240 141.37747192382812 -6.73658561706543 93.79977416992188 100000 -0.18447110056877136 -6.483799934387207 tb/pentd36\n2260 134.1571807861328 -6.994563579559326 45.24525451660156 100000 0.2002066671848297 -6.88761568069458 tb/pentd36\n2280 142.1846923828125 -7.355551719665527 57.082305908203125 100000 -1.4290052652359009 -6.544983863830566 tb/pentd36\n2300 139.1342315673828 -6.885580539703369 0.9616848826408386 100000 -1.4446229934692383 -7.6003499031066895 tb/pentd36\n2320 138.50083923339844 -7.033878326416016 100.21704864501953 100000 0.0459820032119751 -6.930227756500244 tb/pentd36\n2340 138.99110412597656 -7.192724227905273 33.25760269165039 100000 0.36594444513320923 -6.726613521575928 tb/pentd36\n2360 141.42935180664062 -7.411029815673828 80.7456283569336 100000 -1.2937699556350708 -6.656739711761475 tb/pentd36\n2380 140.13906860351562 -6.330612659454346 87.11248779296875 100000 0.5602759122848511 -7.38686990737915 tb/pentd36\n2400 141.39413452148438 -7.4714555740356445 0.3346642255783081 100000 -1.7400935888290405 -7.0007405281066895 tb/pentd36\n198 /20002420 143.639892578125 -7.049715995788574 1.4591994285583496 100000 0.33469223976135254 -7.266991138458252 tb/pentd36\n2440 142.39260864257812 -7.3275933265686035 36.16157150268555 100000 -1.6164711713790894 -7.2901692390441895 tb/pentd36\n2460 142.64938354492188 -7.115324020385742 21.077106475830078 100000 -1.0814917087554932 -7.615800857543945 tb/pentd36\n2480 142.86294555664062 -7.261322975158691 31.723005294799805 100000 0.026784367859363556 -7.291546821594238 tb/pentd36\n2500 139.64306640625 -6.569186687469482 49.5928955078125 100000 -0.6742889881134033 -7.4211506843566895 tb/pentd36\n2520 141.51881408691406 -7.638674736022949 18.553913116455078 100000 -0.18246448040008545 -7.693792343139648 tb/pentd36\n2540 141.68746948242188 -6.704458713531494 16.52964973449707 100000 -0.11814633011817932 -7.448592185974121 tb/pentd36\n2560 139.7581024169922 -7.345004081726074 94.03845977783203 100000 -0.23527011275291443 -5.760656833648682 tb/pentd36\n2580 146.19801330566406 -7.185209274291992 16.164552688598633 100000 -1.7809523344039917 -6.459958076477051 tb/pentd36\n2600 142.13665771484375 -7.364459991455078 35.76545333862305 100000 -0.8366331458091736 -6.80458927154541 tb/pentd36\n198 /20002620 141.14385986328125 -7.089412212371826 35.133323669433594 100000 -0.8298168778419495 -7.487082481384277 tb/pentd36\n2640 142.79624938964844 -6.824095726013184 52.3687629699707 100000 -0.44260650873184204 -7.6817946434021 tb/pentd36\n2660 146.65042114257812 -7.842055797576904 6.394953727722168 100000 -1.1663579940795898 -6.727337837219238 tb/pentd36\n2680 139.39007568359375 -7.456721305847168 17.492584228515625 100000 -1.1137219667434692 -6.712000370025635 tb/pentd36\n2700 139.7080841064453 -6.452831268310547 3.9125170707702637 100000 -0.7041025757789612 -5.9537553787231445 tb/pentd36\n2720 142.19967651367188 -7.267921447753906 58.95460891723633 100000 -0.2923046946525574 -7.310762405395508 tb/pentd36\n2740 137.23907470703125 -6.604380130767822 13.066251754760742 100000 0.2382253259420395 -7.378146171569824 tb/pentd36\n2760 141.15655517578125 -7.268822193145752 33.816341400146484 100000 0.17675045132637024 -7.682333946228027 tb/pentd36\n2780 142.53756713867188 -7.01408576965332 44.10960006713867 100000 0.12659354507923126 -6.860925197601318 tb/pentd36\n2800 139.0703887939453 -6.212903022766113 30.879772186279297 100000 0.3160310685634613 -5.846435546875 tb/pentd36\n198 /20002820 141.494140625 -7.064892768859863 8.671516418457031 100000 -0.24862149357795715 -7.395124435424805 tb/pentd36\n2840 140.6201629638672 -7.338674068450928 127.84257507324219 100000 -1.7561469078063965 -6.560793399810791 tb/pentd36\n2860 138.8127899169922 -7.036996841430664 160.22567749023438 100000 -1.146425724029541 -7.101768970489502 tb/pentd36\n2880 139.32876586914062 -7.103418350219727 133.6594696044922 100000 0.35280951857566833 -6.263919830322266 tb/pentd36\n2900 138.947265625 -6.9614081382751465 61.65150833129883 100000 0.5863454341888428 -7.437838554382324 tb/pentd36\n2920 139.29705810546875 -7.176441669464111 127.63336944580078 100000 -1.1934595108032227 -6.499939441680908 tb/pentd36\n2940 142.4850616455078 -7.487894058227539 84.73721313476562 100000 -0.4463154971599579 -5.996119022369385 tb/pentd36\n2960 145.06976318359375 -7.4152631759643555 40.04515075683594 100000 -0.44624122977256775 -7.064330101013184 tb/pentd36\n2980 146.38868713378906 -7.671492099761963 45.098628997802734 100000 -1.2219082117080688 -6.175599575042725 tb/pentd36\n3000 139.42034912109375 -7.0214643478393555 32.14044189453125 100000 0.3248995244503021 -6.37661600112915 tb/pentd36\n198 /20003020 144.92465209960938 -6.891078472137451 152.803466796875 100000 -1.211337685585022 -7.428390979766846 tb/pentd36\n3040 144.01593017578125 -7.2675981521606445 20.032859802246094 100000 -0.24802933633327484 -7.4731292724609375 tb/pentd36\n3060 139.36358642578125 -6.950523376464844 127.95497131347656 100000 0.24874837696552277 -7.7955851554870605 tb/pentd36\n3080 140.83505249023438 -6.8481855392456055 31.314579010009766 100000 0.260641872882843 -7.195136070251465 tb/pentd36\n3100 138.36024475097656 -6.728329658508301 59.648929595947266 100000 -0.5832265615463257 -7.354883670806885 tb/pentd36\n3120 139.34335327148438 -7.171263217926025 207.0855712890625 100000 0.2678053081035614 -7.519973278045654 tb/pentd36\n3140 143.9208984375 -7.0992231369018555 53.12328338623047 100000 -1.4692577123641968 -7.553769588470459 tb/pentd36\n3160 140.06536865234375 -6.991345405578613 8.59929084777832 100000 0.05494752153754234 -7.394230842590332 tb/pentd36\n3180 140.6368865966797 -6.833036422729492 18.766809463500977 100000 -0.9616910219192505 -7.422491073608398 tb/pentd36\n3200 145.88442993164062 -6.1915812492370605 37.074859619140625 100000 0.28312188386917114 -7.262149333953857 tb/pentd36\n198 /20003220 145.07388305664062 -7.357606410980225 97.8402099609375 100000 -0.581906259059906 -7.546910762786865 tb/pentd36\n3240 145.8331298828125 -7.083222389221191 5.207602500915527 100000 0.6407988667488098 -7.250114917755127 tb/pentd36\n3260 144.42031860351562 -7.074912071228027 124.3154296875 100000 0.06015363335609436 -6.652688980102539 tb/pentd36\n3280 145.16934204101562 -7.878324508666992 107.5687484741211 100000 0.4795902967453003 -7.461205005645752 tb/pentd36\n3300 143.9312744140625 -7.468573093414307 9.323836326599121 100000 -0.03496647998690605 -5.995175361633301 tb/pentd36\n3320 146.6934051513672 -6.737509727478027 13.793230056762695 100000 0.3884173631668091 -7.731751441955566 tb/pentd36\n3340 139.89788818359375 -6.792123794555664 21.593040466308594 100000 0.41134336590766907 -6.7290167808532715 tb/pentd36\n3360 140.23843383789062 -7.068183422088623 10.301061630249023 100000 -0.6076937913894653 -7.310390472412109 tb/pentd36\n3380 142.5211639404297 -7.296858310699463 30.497098922729492 100000 -1.3474717140197754 -6.022459506988525 tb/pentd36\n3400 140.9469757080078 -7.093663692474365 20.28019905090332 100000 0.034556079655885696 -7.578153610229492 tb/pentd36\n198 /20003420 141.9439697265625 -7.032241344451904 4.646529674530029 100000 0.0029428431298583746 -7.372579097747803 tb/pentd36\n3440 141.84320068359375 -6.381721496582031 0.42640385031700134 100000 -1.7557204961776733 -6.578649044036865 tb/pentd36\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-43a945ef3444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_policy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbuff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mwarmup\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaxeract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0moldobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mrbuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moldobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi/rew\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mujoco-py/myrl/environments.py\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(self, pi, gamma, length, debug)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mujoco-py/myrl/environments.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, pi, debug)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mujoco-py/myrl/environments.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, act, sample, debug)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0masq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0masq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-43a945ef3444>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(obs)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mwarmup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmaxeract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmaxer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mujoco-py/myrl/policies.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, o, std, debug)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mujoco-py/myrl/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0modim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wll.new()\n",
    "writer = wll.writer\n",
    "for i in range(1):\n",
    "    eps = 10000\n",
    "    gamma = 0.95\n",
    "    bsize = 128\n",
    "    warmup = 5000\n",
    "    maxeract = lambda obs: maxer.act(obs, std=0.3)\n",
    "\n",
    "    for ep in range(0, eps):\n",
    "\n",
    "        for i in range(1):\n",
    "            pi = random_policy if len(rbuff)<warmup else maxeract\n",
    "            oldobs, a, r, obs, d, _, _, _ = envs.rollout(pi, debug=0)\n",
    "            rbuff.add(oldobs, a, r, obs, d)\n",
    "        writer.add_scalar(\"multi/rew\", r.mean().item(), ep)\n",
    "        rreal = r.mean().item()\n",
    "        \n",
    "\n",
    "        if len(rbuff) < bsize:\n",
    "            print(len(rbuff), end='\\r')\n",
    "            continue  \n",
    "\n",
    "        for put in range(20):\n",
    "            # print(len(rbuff), bsize)\n",
    "            # print(len(rbuff.deqs[0]))\n",
    "            oldobs, a, r, obs, d = rbuff.get(bsize)\n",
    "\n",
    "\n",
    "            for optstep in range(2):\n",
    "                amax = maxer(obs).detach()\n",
    "                amax += normal_noise(amax, 0.08)\n",
    "                in1 = torch.cat((obs, amax), dim=-1)\n",
    "                in2 = torch.cat((oldobs, a), dim=-1)\n",
    "                target = r + gamma*(1-d*0)*torch.min(tq1(in1), tq2(in1))[0]\n",
    "                td_error = ((target - q1(in2))**2) + ((target - q2(in2))**2)\n",
    "                td = td_error.mean()\n",
    "                opt.zero_grad()\n",
    "                td.backward()\n",
    "                opt.step()\n",
    "            # rbuff.add(oldobs, a, r, obs, d)#, vals=-td_error)\n",
    "            \n",
    "            if not put%2 or 1:\n",
    "                for optstep in range(4):\n",
    "                    amax = maxer(obs)\n",
    "                    in1 = torch.cat((obs, amax), dim=-1)\n",
    "                    maxer_loss = -q1(in1).mean()\n",
    "                    mopt.zero_grad()\n",
    "                    maxer_loss.backward()\n",
    "                    clip_grad_norm_(mopt, 0.5)\n",
    "                    mopt.step()\n",
    "\n",
    "            polyak(q1, tq1, 1-1/70)\n",
    "            polyak(q2, tq2, 1-1/70)\n",
    "\n",
    "        writer.add_scalar(\"multi/maxer_loss\", maxer_loss.item(), ep)\n",
    "        writer.add_scalar(\"multi/td_q\", td.item(), ep)\n",
    "\n",
    "        if ep%20==0:\n",
    "            print(ep, maxer_loss.item(), r.mean().item(), td.item(), len(rbuff), chck(maxer).item(), rreal, wll.s+str(wll.z))\n",
    "        if ep%200==0:\n",
    "            showit(env, maxer.act, )\n",
    "            env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(58373, 58340, list)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "len(rbuff), len(rbuff.minheap.heap), type(rbuff.minheap.heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0000,  0.0000]]),\n  tensor([[0.0061, 0.0943]]),\n  tensor([[-0.1348]]),\n  tensor([[ 0.0172,  1.4702,  0.1591,  0.1603, -0.0201, -0.0367,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2980748414993286,\n  tensor([[ 0.8366,  0.4229,  1.0289, -0.7017, -0.2128, -0.0089,  0.0000,  0.0000]]),\n  tensor([[-0.0011,  0.3590]]),\n  tensor([[-1.7647]]),\n  tensor([[ 0.8469,  0.4065,  1.0289, -0.7284, -0.2132, -0.0089,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.3040266036987305,\n  tensor([[ 0.6615,  0.3700,  1.2344, -0.6874, -0.4556, -0.0880,  0.0000,  0.0000]]),\n  tensor([[0.0196, 0.2994]]),\n  tensor([[-3.2734]]),\n  tensor([[ 0.6740,  0.3543,  1.2567, -0.6960, -0.4596, -0.0802,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.3010417222976685,\n  tensor([[-0.2988,  0.0552, -0.3717,  0.0056, -0.0052,  0.0080,  0.0000,  0.0000]]),\n  tensor([[0.0951, 0.3398]]),\n  tensor([[-0.3845]]),\n  tensor([[-0.3025,  0.0552, -0.3707, -0.0038, -0.0048,  0.0088,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.303085207939148,\n  tensor([[-0.0561,  0.6019, -0.6807, -0.3177,  0.4512,  0.1111,  0.0000,  0.0000]]),\n  tensor([[0.1140, 0.3911]]),\n  tensor([[-2.5671]]),\n  tensor([[-0.0631,  0.5949, -0.7094, -0.3153,  0.4565,  0.1045,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.299889087677002,\n  tensor([[ 0.0822,  0.9259,  0.2432, -0.5364, -0.1109, -0.0636,  0.0000,  0.0000]]),\n  tensor([[0.1795, 0.2819]]),\n  tensor([[0.2199]]),\n  tensor([[ 0.0846,  0.9137,  0.2389, -0.5432, -0.1144, -0.0710,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.297675609588623,\n  tensor([[ 0.1855,  1.0773, -0.0380, -0.7421,  0.3866,  0.1478,  0.0000,  0.0000]]),\n  tensor([[ 0.5800, -0.6831]]),\n  tensor([[1.3413]]),\n  tensor([[ 0.1850,  1.0609, -0.0568, -0.7321,  0.3956,  0.1801,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.3032684326171875,\n  tensor([[-0.2329,  1.5494, -0.1719, -0.0092, -0.0250,  0.0067,  0.0000,  0.0000]]),\n  tensor([[0.1014, 0.1791]]),\n  tensor([[-0.4822]]),\n  tensor([[-0.2346,  1.5493, -0.1752, -0.0036, -0.0249,  0.0023,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.3029800653457642,\n  tensor([[ 0.2772,  1.5151,  0.8628,  0.0140, -0.3051, -0.1522,  0.0000,  0.0000]]),\n  tensor([[0.0007, 0.0179]]),\n  tensor([[-1.5436]]),\n  tensor([[ 2.8581e-01,  1.5151e+00,  8.6762e-01, -1.0850e-03, -3.1269e-01,\n           -1.5279e-01,  0.0000e+00,  0.0000e+00]]),\n  tensor([[0.]])],\n [-1.2987284660339355,\n  tensor([[-0.8335,  1.3844, -1.5684, -0.0805,  0.5435,  0.0813,  0.0000,  0.0000]]),\n  tensor([[0.2327, 0.4466]]),\n  tensor([[-3.9488]]),\n  tensor([[-0.8494,  1.3826, -1.5954, -0.0821,  0.5474,  0.0777,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.3000057935714722,\n  tensor([[ 0.2029,  1.0534,  0.3802, -0.2154, -0.1372, -0.0280,  0.0000,  0.0000]]),\n  tensor([[0.0628, 0.1780]]),\n  tensor([[0.2583]]),\n  tensor([[ 0.2067,  1.0486,  0.3812, -0.2096, -0.1388, -0.0335,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.301077961921692,\n  tensor([[-0.3241,  0.3223, -0.6381, -1.0768,  0.1163, -0.1033,  0.0000,  0.0000]]),\n  tensor([[0.6038, 0.4558]]),\n  tensor([[3.7896]]),\n  tensor([[-0.3305,  0.2987, -0.6343, -1.0511,  0.1117, -0.0908,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.300576090812683,\n  tensor([[-0.0332,  1.0659,  0.0066, -0.2660, -0.1147, -0.0491,  0.0000,  0.0000]]),\n  tensor([[0.1132, 0.2567]]),\n  tensor([[-0.6697]]),\n  tensor([[-0.0332,  1.0597,  0.0047, -0.2745, -0.1174, -0.0540,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.302419900894165,\n  tensor([[-0.2216,  0.7445, -0.5100, -0.4410,  0.1417,  0.0130,  0.0000,  0.0000]]),\n  tensor([[0.1895, 0.4143]]),\n  tensor([[-0.0249]]),\n  tensor([[-0.2268,  0.7347, -0.5235, -0.4343,  0.1421,  0.0068,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.3018158674240112,\n  tensor([[ 0.1277,  1.2557,  0.3374, -0.2114, -0.1053, -0.0426,  0.0000,  0.0000]]),\n  tensor([[0.0846, 0.1583]]),\n  tensor([[-1.0258]]),\n  tensor([[ 0.1311,  1.2506,  0.3431, -0.2235, -0.1072, -0.0393,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.3019793033599854,\n  tensor([[ 0.1919,  1.4893,  0.6303,  0.0650, -0.2116, -0.1336,  0.0000,  0.0000]]),\n  tensor([[0.0028, 0.0330]]),\n  tensor([[-2.4804]]),\n  tensor([[ 0.1983,  1.4907,  0.6450,  0.0653, -0.2180, -0.1279,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.3008639812469482,\n  tensor([[-0.1981,  1.6016, -0.2194, -0.0504, -0.0042, -0.0081,  0.0000,  0.0000]]),\n  tensor([[0.1076, 0.1887]]),\n  tensor([[-0.9660]]),\n  tensor([[-0.2004,  1.6003, -0.2258, -0.0589, -0.0050, -0.0145,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2990565299987793,\n  tensor([[-0.2881,  1.5321, -0.1391, -0.1061, -0.0346,  0.0269,  0.0000,  0.0000]]),\n  tensor([[0.1403, 0.1918]]),\n  tensor([[1.1288]]),\n  tensor([[-0.2895,  1.5300, -0.1364, -0.0930, -0.0333,  0.0275,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2969326972961426,\n  tensor([[-0.6336,  1.2957, -1.3298, -0.0486,  0.6112,  0.1422,  0.0000,  0.0000]]),\n  tensor([[0.2018, 0.4010]]),\n  tensor([[-3.1952]]),\n  tensor([[-0.6470,  1.2947, -1.3474, -0.0485,  0.6187,  0.1506,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.3004015684127808,\n  tensor([[-0.0239,  1.3727, -0.4781, -0.3603,  0.0293,  0.1235,  0.0000,  0.0000]]),\n  tensor([[0.1312, 0.3096]]),\n  tensor([[-0.6799]]),\n  tensor([[-0.0286,  1.3643, -0.4798, -0.3701,  0.0354,  0.1226,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2977681159973145,\n  tensor([[-0.0054,  0.6201, -0.0223, -0.8410,  0.0444,  0.0121,  0.0000,  0.0000]]),\n  tensor([[-0.2866, -0.6626]]),\n  tensor([[-0.9898]]),\n  tensor([[-0.0057,  0.6006, -0.0302, -0.8678,  0.0465,  0.0438,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.301701307296753,\n  tensor([[-0.1271,  1.5135, -0.3529,  0.0396,  0.1283,  0.0611,  0.0000,  0.0000]]),\n  tensor([[0.0741, 0.2047]]),\n  tensor([[-0.2425]]),\n  tensor([[-0.1305,  1.5141, -0.3506,  0.0274,  0.1316,  0.0660,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2986434698104858,\n  tensor([[-0.2427,  0.9890, -0.5713, -0.3708,  0.1983,  0.0921,  0.0000,  0.0000]]),\n  tensor([[0.1946, 0.3764]]),\n  tensor([[-1.3910]]),\n  tensor([[-0.2485,  0.9807, -0.5884, -0.3712,  0.2024,  0.0833,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2979143857955933,\n  tensor([[ 0.5219,  1.0608,  1.1402, -0.3424, -0.4813, -0.0942,  0.0000,  0.0000]]),\n  tensor([[0.0181, 0.1052]]),\n  tensor([[-2.6303]]),\n  tensor([[ 0.5335,  1.0529,  1.1604, -0.3524, -0.4857, -0.0880,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2962408065795898,\n  tensor([[-0.2905,  1.2755, -0.3872, -0.1283, -0.0257, -0.0607,  0.0000,  0.0000]]),\n  tensor([[0.1293, 0.2760]]),\n  tensor([[-0.2979]]),\n  tensor([[-0.2944,  1.2728, -0.3892, -0.1216, -0.0289, -0.0640,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2964270114898682,\n  tensor([[ 0.0939,  0.0328,  0.2424, -0.0418, -0.2883, -0.1372,  0.0000,  0.0000]]),\n  tensor([[0.0098, 0.2534]]),\n  tensor([[-2.8162]]),\n  tensor([[ 0.0965,  0.0321,  0.2613, -0.0361, -0.2949, -0.1329,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2975398302078247,\n  tensor([[ 0.0723,  1.4459,  0.2124, -0.0024, -0.0559, -0.0395,  0.0000,  0.0000]]),\n  tensor([[0.0375, 0.1160]]),\n  tensor([[-1.2337]]),\n  tensor([[ 0.0745,  1.4456,  0.2214, -0.0129, -0.0575, -0.0321,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2998969554901123,\n  tensor([[ 0.2113,  1.5399,  0.6564,  0.0524, -0.2116, -0.1300,  0.0000,  0.0000]]),\n  tensor([[0.0040, 0.0261]]),\n  tensor([[-1.6596]]),\n  tensor([[ 0.2179,  1.5411,  0.6627,  0.0530, -0.2182, -0.1324,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.298059344291687,\n  tensor([[ 0.7653,  0.8187,  1.3373, -0.4848, -0.5091, -0.1325,  0.0000,  0.0000]]),\n  tensor([[-0.0019,  0.2020]]),\n  tensor([[-1.6788]]),\n  tensor([[ 0.7786,  0.8073,  1.3373, -0.5115, -0.5157, -0.1325,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.297160267829895,\n  tensor([[-0.1266,  1.1922, -0.7435, -0.5049,  0.1340,  0.1521,  0.0000,  0.0000]]),\n  tensor([[0.1864, 0.3776]]),\n  tensor([[0.4064]]),\n  tensor([[-0.1340,  1.1810, -0.7439, -0.4982,  0.1420,  0.1583,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.3036390542984009,\n  tensor([[-0.1653,  0.6684, -0.2324, -0.6267, -0.0332,  0.0124,  0.0000,  0.0000]]),\n  tensor([[0.4462, 0.8327]]),\n  tensor([[3.0830]]),\n  tensor([[-0.1676,  0.6548, -0.2329, -0.6031, -0.0344, -0.0238,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.3024027347564697,\n  tensor([[ 0.1563,  1.5599,  0.3290,  0.0358, -0.0966, -0.0616,  0.0000,  0.0000]]),\n  tensor([[0.0163, 0.0753]]),\n  tensor([[-1.1210]]),\n  tensor([[ 0.1597,  1.5605,  0.3355,  0.0287, -0.0995, -0.0581,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2995928525924683,\n  tensor([[ 0.0192,  1.4753,  0.2771,  0.3289, -0.0218, -0.0620,  0.0000,  0.0000]]),\n  tensor([[-0.0317,  0.0440]]),\n  tensor([[1.0088]]),\n  tensor([[ 0.0219,  1.4821,  0.2771,  0.3022, -0.0249, -0.0620,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.300052285194397,\n  tensor([[ 0.0855,  1.6467,  0.2597,  0.1020, -0.1551, -0.0404,  0.0000,  0.0000]]),\n  tensor([[0.0013, 0.0636]]),\n  tensor([[-0.9276]]),\n  tensor([[ 0.0881,  1.6488,  0.2665,  0.0942, -0.1570, -0.0382,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2957203388214111,\n  tensor([[ 0.6470,  1.4433,  1.2744, -0.3510, -0.6974, -0.1555,  0.0000,  0.0000]]),\n  tensor([[0.0020, 0.0659]]),\n  tensor([[-1.9060]]),\n  tensor([[ 0.6597,  1.4351,  1.2825, -0.3653, -0.7055, -0.1610,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.3024966716766357,\n  tensor([[ 0.7285,  0.6656,  0.9428, -0.4828, -0.2145, -0.0156,  0.0000,  0.0000]]),\n  tensor([[0.0019, 0.2715]]),\n  tensor([[-0.9582]]),\n  tensor([[ 0.7380,  0.6548,  0.9524, -0.4804, -0.2153, -0.0156,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2980436086654663,\n  tensor([[-0.4251,  1.5255, -0.9099, -0.0134,  0.4772,  0.1471,  0.0000,  0.0000]]),\n  tensor([[0.1842, 0.3019]]),\n  tensor([[-4.0527]]),\n  tensor([[-0.4344,  1.5253, -0.9393, -0.0119,  0.4842,  0.1406,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2997013330459595,\n  tensor([[-0.0653,  1.5628, -0.2002, -0.0278,  0.0815, -0.0054,  0.0000,  0.0000]]),\n  tensor([[0.0817, 0.1836]]),\n  tensor([[-0.5253]]),\n  tensor([[-0.0673,  1.5621, -0.2046, -0.0294,  0.0812, -0.0066,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2930757999420166,\n  tensor([[ 0.1375,  1.1485,  0.4858, -0.3480, -0.1935, -0.1039,  0.0000,  0.0000]]),\n  tensor([[0.1220, 0.1793]]),\n  tensor([[-0.8023]]),\n  tensor([[ 0.1424,  1.1409,  0.5027, -0.3383, -0.1984, -0.0979,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.279971718788147,\n  tensor([[ 0.4832,  1.2279,  0.6521, -0.2940, -0.0930,  0.0163,  0.0000,  0.0000]]),\n  tensor([[0.0348, 0.1356]]),\n  tensor([[-0.2848]]),\n  tensor([[ 0.4898,  1.2214,  0.6610, -0.2885, -0.0920,  0.0205,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.302475094795227,\n  tensor([[-4.3854e-01,  1.4992e+00, -7.2934e-01, -6.1320e-02,  2.0796e-01,\n           -6.7439e-04,  0.0000e+00,  0.0000e+00]]),\n  tensor([[0.1825, 0.2775]]),\n  tensor([[-2.0440]]),\n  tensor([[-0.4460,  1.4980, -0.7481, -0.0531,  0.2076, -0.0080,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2829622030258179,\n  tensor([[-0.4576,  0.6756, -0.9398, -0.3231,  0.3603,  0.1226,  0.0000,  0.0000]]),\n  tensor([[0.1870, 0.4530]]),\n  tensor([[-2.5360]]),\n  tensor([[-0.4671,  0.6682, -0.9574, -0.3287,  0.3661,  0.1175,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.300321340560913,\n  tensor([[ 0.4596,  1.2954,  0.9358, -0.2057, -0.3375, -0.0676,  0.0000,  0.0000]]),\n  tensor([[0.0192, 0.0763]]),\n  tensor([[-2.2547]]),\n  tensor([[ 0.4692,  1.2907,  0.9543, -0.2109, -0.3405, -0.0610,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2938038110733032,\n  tensor([[ 0.7449,  0.1255,  0.7135,  0.0586, -1.0116, -2.5194,  1.0000,  0.0000]]),\n  tensor([[0.1211, 0.5475]]),\n  tensor([[-27.9957]]),\n  tensor([[ 0.7519,  0.1281,  0.7583,  0.0397, -1.1390, -2.5483,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2912895679473877,\n  tensor([[-0.1545,  1.4251, -0.4369, -0.0079,  0.1361,  0.0592,  0.0000,  0.0000]]),\n  tensor([[0.0875, 0.2403]]),\n  tensor([[-1.0519]]),\n  tensor([[-0.1589,  1.4248, -0.4426, -0.0145,  0.1389,  0.0578,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.296204686164856,\n  tensor([[ 0.1462,  1.2001,  0.3120, -0.1740, -0.0324,  0.0183,  0.0000,  0.0000]]),\n  tensor([[0.0685, 0.1561]]),\n  tensor([[-0.5041]]),\n  tensor([[ 0.1494,  1.1960,  0.3181, -0.1800, -0.0313,  0.0233,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.293401837348938,\n  tensor([[ 0.3964,  0.8455,  0.1600, -0.2372,  0.2865,  0.1250,  0.0000,  0.0000]]),\n  tensor([[0.0298, 0.2464]]),\n  tensor([[0.3514]]),\n  tensor([[ 0.3980,  0.8403,  0.1511, -0.2342,  0.2930,  0.1289,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2987747192382812,\n  tensor([[ 0.5697,  0.8307,  1.2585, -0.5285, -0.5700, -0.2050,  0.0000,  0.0000]]),\n  tensor([[0.0468, 0.1673]]),\n  tensor([[-2.6195]]),\n  tensor([[ 0.5824,  0.8189,  1.2762, -0.5287, -0.5806, -0.2113,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.296868085861206,\n  tensor([[ 0.1504,  1.5074,  0.5808,  0.0877, -0.1538, -0.0993,  0.0000,  0.0000]]),\n  tensor([[0.0059, 0.0274]]),\n  tensor([[-1.9765]]),\n  tensor([[ 0.1563,  1.5094,  0.5922,  0.0860, -0.1585, -0.0939,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2958660125732422,\n  tensor([[ 0.1146,  1.4254,  0.5197, -0.0560, -0.1635, -0.0854,  0.0000,  0.0000]]),\n  tensor([[0.2595, 0.1059]]),\n  tensor([[-1.4356]]),\n  tensor([[ 0.1198,  1.4239,  0.5279, -0.0683, -0.1675, -0.0810,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2982046604156494,\n  tensor([[-0.0289,  1.1848, -0.1377, -0.4726,  0.0218,  0.0200,  0.0000,  0.0000]]),\n  tensor([[0.1806, 0.2977]]),\n  tensor([[1.7558]]),\n  tensor([[-0.0303,  1.1744, -0.1345, -0.4629,  0.0230,  0.0243,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2887725830078125,\n  tensor([[ 0.4881,  1.0528,  0.9646, -0.3567, -0.4269, -0.0914,  0.0000,  0.0000]]),\n  tensor([[0.0307, 0.1290]]),\n  tensor([[-2.3096]]),\n  tensor([[ 0.4979,  1.0446,  0.9832, -0.3658, -0.4312, -0.0857,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2952078580856323,\n  tensor([[ 0.0358,  1.4713,  0.5176,  0.3032, -0.0407, -0.1159,  0.0000,  0.0000]]),\n  tensor([[-0.0182,  0.0059]]),\n  tensor([[0.0865]]),\n  tensor([[ 0.0409,  1.4775,  0.5176,  0.2766, -0.0465, -0.1159,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.29429030418396,\n  tensor([[ 0.7754,  1.3757,  1.3469, -0.4017, -0.5208, -0.0576,  0.0000,  0.0000]]),\n  tensor([[0.0015, 0.1021]]),\n  tensor([[-3.0188]]),\n  tensor([[ 0.7891,  1.3667,  1.3749, -0.4027, -0.5234, -0.0527,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2992932796478271,\n  tensor([[ 0.1387,  0.9698,  0.2410, -0.3111, -0.0508, -0.0197,  0.0000,  0.0000]]),\n  tensor([[0.1042, 0.2300]]),\n  tensor([[0.0969]]),\n  tensor([[ 0.1411,  0.9628,  0.2463, -0.3109, -0.0516, -0.0165,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2976073026657104,\n  tensor([[-0.9714,  1.1938, -1.9195, -0.1690,  0.6392, -0.0119,  0.0000,  0.0000]]),\n  tensor([[0.2121, 0.5538]]),\n  tensor([[-3.5061]]),\n  tensor([[-0.9909,  1.1902, -1.9456, -0.1594,  0.6375, -0.0340,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2962863445281982,\n  tensor([[ 0.1395,  1.4671,  0.5912,  0.0786, -0.1596, -0.1239,  0.0000,  0.0000]]),\n  tensor([[0.0113, 0.0354]]),\n  tensor([[-1.7706]]),\n  tensor([[ 0.1455,  1.4687,  0.5999,  0.0729, -0.1656, -0.1204,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2937091588974,\n  tensor([[ 0.0145,  1.4442,  0.3658,  0.3303, -0.0165, -0.0820,  0.0000,  0.0000]]),\n  tensor([[-0.0262,  0.0320]]),\n  tensor([[0.6481]]),\n  tensor([[ 0.0181,  1.4511,  0.3659,  0.3036, -0.0206, -0.0819,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2974095344543457,\n  tensor([[ 0.2238,  1.4920,  0.6671,  0.0352, -0.1757, -0.0814,  0.0000,  0.0000]]),\n  tensor([[0.0046, 0.0281]]),\n  tensor([[-0.6086]]),\n  tensor([[ 0.2304,  1.4927,  0.6657,  0.0314, -0.1801, -0.0886,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2768678665161133,\n  tensor([[ 0.0062,  1.2021, -0.0111, -0.4663, -0.0168, -0.0017,  0.0000,  0.0000]]),\n  tensor([[ 0.3577, -0.8870]]),\n  tensor([[0.7423]]),\n  tensor([[ 0.0059,  1.1916, -0.0297, -0.4679, -0.0152,  0.0316,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2990301847457886,\n  tensor([[-0.0841,  1.4389, -0.4529,  0.0168,  0.0934,  0.1102,  0.0000,  0.0000]]),\n  tensor([[0.0548, 0.2332]]),\n  tensor([[-1.0055]]),\n  tensor([[-0.0886,  1.4389, -0.4558,  0.0037,  0.0989,  0.1092,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.292905569076538,\n  tensor([[-0.6534,  1.6370, -1.0362,  0.0896,  0.2701,  0.0374,  0.0000,  0.0000]]),\n  tensor([[0.1737, 0.3019]]),\n  tensor([[-2.3801]]),\n  tensor([[-0.6639,  1.6388, -1.0522,  0.0786,  0.2715,  0.0285,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2912427186965942,\n  tensor([[-0.3806,  1.3379, -0.2044, -0.1244,  0.0284, -0.0160,  0.0000,  0.0000]]),\n  tensor([[0.1682, 0.2357]]),\n  tensor([[0.2551]]),\n  tensor([[-0.3826,  1.3351, -0.2028, -0.1242,  0.0277, -0.0133,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2950913906097412,\n  tensor([[-0.0305,  1.5412, -0.1003,  0.0449,  0.0653,  0.0303,  0.0000,  0.0000]]),\n  tensor([[0.0536, 0.1524]]),\n  tensor([[-0.6279]]),\n  tensor([[-0.0316,  1.5419, -0.1081,  0.0318,  0.0665,  0.0240,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2956310510635376,\n  tensor([[ 0.0137,  0.7224, -0.7605, -0.3756,  0.3710,  0.0433,  0.0000,  0.0000]]),\n  tensor([[0.1171, 0.3905]]),\n  tensor([[-1.3064]]),\n  tensor([[ 0.0060,  0.7137, -0.7757, -0.3861,  0.3729,  0.0381,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2888481616973877,\n  tensor([[ 0.9495,  0.9848,  1.6190, -0.6961, -0.8118, -0.1855,  0.0000,  0.0000]]),\n  tensor([[-0.0175,  0.2287]]),\n  tensor([[-1.9722]]),\n  tensor([[ 0.9656,  0.9686,  1.6190, -0.7227, -0.8211, -0.1855,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.288543462753296,\n  tensor([[-0.3292,  1.4988, -0.9215,  0.0196,  0.2885,  0.1372,  0.0000,  0.0000]]),\n  tensor([[0.1173, 0.2982]]),\n  tensor([[-1.3445]]),\n  tensor([[-0.3384,  1.4991, -0.9240,  0.0112,  0.2957,  0.1424,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2913250923156738,\n  tensor([[-0.2465,  1.4109, -0.5762, -0.0701,  0.1546,  0.0570,  0.0000,  0.0000]]),\n  tensor([[0.1253, 0.2729]]),\n  tensor([[-1.5229]]),\n  tensor([[-0.2524,  1.4094, -0.5882, -0.0656,  0.1572,  0.0525,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2875676155090332,\n  tensor([[ 0.6454,  1.4093,  1.2892, -0.2847, -0.5779, -0.1611,  0.0000,  0.0000]]),\n  tensor([[0.0015, 0.0650]]),\n  tensor([[-3.0105]]),\n  tensor([[ 0.6584,  1.4029,  1.3097, -0.2882, -0.5860, -0.1628,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.299822449684143,\n  tensor([[ 0.5432,  1.5167,  1.2012, -0.1626, -0.4426, -0.1350,  0.0000,  0.0000]]),\n  tensor([[0.0016, 0.0349]]),\n  tensor([[-2.1567]]),\n  tensor([[ 0.5552,  1.5130,  1.2133, -0.1658, -0.4495, -0.1390,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2889008522033691,\n  tensor([[ 0.0349,  1.2600,  0.2188, -0.5001, -0.0426, -0.0280,  0.0000,  0.0000]]),\n  tensor([[-0.4615, -0.0098]]),\n  tensor([[-1.4153]]),\n  tensor([[ 0.0371,  1.2481,  0.2188, -0.5268, -0.0440, -0.0280,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.296149730682373,\n  tensor([[ 0.3485,  1.1150,  0.9273, -0.2956, -0.3121, -0.1257,  0.0000,  0.0000]]),\n  tensor([[0.0554, 0.1146]]),\n  tensor([[-1.7884]]),\n  tensor([[ 0.3579,  1.1085,  0.9432, -0.2895, -0.3184, -0.1257,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2975280284881592,\n  tensor([[ 0.5320,  1.3987,  0.5910, -0.2066, -0.0738,  0.0235,  0.0000,  0.0000]]),\n  tensor([[0.0072, 0.1157]]),\n  tensor([[0.0412]]),\n  tensor([[ 0.5380,  1.3942,  0.5937, -0.2032, -0.0727,  0.0228,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2917698621749878,\n  tensor([[ 0.4328, -0.0667, -0.2336,  0.0130,  0.1664, -0.2219,  0.0000,  0.0000]]),\n  tensor([[0.0586, 0.3086]]),\n  tensor([[0.3155]]),\n  tensor([[ 0.4303, -0.0662, -0.2419,  0.0215,  0.1554, -0.2213,  0.0000,  0.0000]]),\n  tensor([[0.]])],\n [-1.2959376573562622,\n  tensor([[ 0.4011, -0.0661, -0.0070, -0.0077, -0.0096, -0.0074,  0.0000,  0.0000]]),\n  tensor([[-0.0065,  0.2988]]),\n  tensor([[-2.5083]]),\n  tensor([[ 0.4010, -0.0669, -0.0070, -0.0344, -0.0099, -0.0074,  0.0000,  0.0000]]),\n  tensor([[0.]])]]"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "rbuff.minheap.heap[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-1487d8932d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mheappop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminheap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# rbuff.add(oldobs, a, r, obs, d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "from heapq import heappop\n",
    "for i in range(1000):\n",
    "    print(i, end='\\r')\n",
    "    \n",
    "    heappop(rbuff.minheap.heap[:3])\n",
    "    # rbuff.add(oldobs, a, r, obs, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldobs, a, r, obs, d = rbuff.get(bsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-80f78089d33b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mheapq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheappop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mheappop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminheap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "from heapq import heappop\n",
    "heappop(rbuff.minheap.heap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4167480469\n80.78876495361328\n77.13992309570312\n76.94164276123047\n84.19696044921875\n77.95185852050781\n77.52365112304688\n79.0836410522461\n79.05925750732422\n105.01483917236328\n120.8262939453125\n127.11097717285156\n138.15594482421875\n139.5283660888672\n126.37893676757812\n138.97605895996094\n108.20635986328125\n126.78392028808594\n126.03907012939453\n105.32546997070312\n110.4582748413086\n138.41761779785156\n138.55613708496094\n103.26380157470703\n107.60101318359375\n107.34993743896484\n124.75959777832031\n114.57164001464844\n111.92471313476562\n121.66626739501953\n138.2199249267578\n140.74972534179688\n138.24034118652344\n141.03289794921875\n133.9188995361328\n138.0693359375\n104.6457748413086\n111.25828552246094\n207.53164672851562\n138.18309020996094\n138.4364471435547\n138.85540771484375\n110.75657653808594\n147.15150451660156\n138.9591827392578\n112.87947845458984\n139.85845947265625\n138.87672424316406\n114.1255874633789\n138.19126892089844\n218.67672729492188\n138.2176971435547\n165.14398193359375\n138.22959899902344\n116.94247436523438\n141.848876953125\n139.1126708984375\n107.13170623779297\n132.06744384765625\n128.74163818359375\n105.41802215576172\n138.2534942626953\n138.1905975341797\n113.19166564941406\n106.01872253417969\n132.76480102539062\n138.5903778076172\n137.7503662109375\n138.43011474609375\n138.61683654785156\n138.18858337402344\n138.5388946533203\n140.3739013671875\n138.18577575683594\n146.49681091308594\n138.16537475585938\n139.0016632080078\n139.15736389160156\n138.24368286132812\n114.47415924072266\n106.17400360107422\n138.29698181152344\n101.61894226074219\n108.86035919189453\n107.60880279541016\n100.57196807861328\n140.6708526611328\n109.62106323242188\n104.3980941772461\n142.980712890625\n140.99000549316406\n138.18576049804688\n138.34457397460938\n104.73990631103516\n127.78746032714844\n139.12271118164062\n138.53225708007812\n120.42855834960938\n106.2403793334961\n104.67227935791016\n113.29733276367188\n138.46783447265625\n138.13885498046875\n138.23770141601562\n138.18309020996094\n114.27941131591797\n100.430908203125\n103.7947998046875\n138.2233428955078\n138.2238006591797\n123.61785888671875\n109.03395080566406\n105.09952545166016\n138.17208862304688\n92.8293228149414\n140.79824829101562\n139.25709533691406\n103.46907806396484\n104.59956359863281\n138.63804626464844\n100.50242614746094\n138.279052734375\n111.9057846069336\n103.37931060791016\n107.25981140136719\n128.89071655273438\n133.51507568359375\n118.83470916748047\n105.64306640625\n118.33316040039062\n100.57434844970703\n139.3179931640625\n138.21998596191406\n138.4776153564453\n138.6253662109375\n108.68729400634766\n138.13893127441406\n101.87629699707031\n105.6224136352539\n115.66709899902344\n138.29464721679688\n138.2997589111328\n141.07553100585938\n118.95682525634766\n138.77073669433594\n129.9833984375\n102.10326385498047\n138.1480255126953\n138.14328002929688\n138.13961791992188\n117.7097396850586\n100.16276550292969\n138.2905731201172\n114.01266479492188\n102.74193572998047\n100.91032409667969\n101.6048812866211\n132.66246032714844\n127.57115173339844\n140.2426300048828\n103.895263671875\n139.62339782714844\n140.89427185058594\n107.96324920654297\n116.0020980834961\n138.48036193847656\n138.1711883544922\n103.56987762451172\n101.53251647949219\n138.4072723388672\n141.4117431640625\n118.10665893554688\n138.74929809570312\n101.03411865234375\n110.45719909667969\n122.54829406738281\n138.1385040283203\n116.8802490234375\n115.38806915283203\n138.18028259277344\n122.86702728271484\n104.92949676513672\n109.09524536132812\n104.2768783569336\n101.95848846435547\n103.73307037353516\n139.0917205810547\n136.4574432373047\n115.84613037109375\n138.7814178466797\n138.13975524902344\n138.5573272705078\n116.79204559326172\n146.73497009277344\n102.89360809326172\n131.6776123046875\n106.1778793334961\n139.38685607910156\n138.56629943847656\n108.89447784423828\n137.03408813476562\n103.13447570800781\n108.63031005859375\n102.2965087890625\n110.25592041015625\n103.57542419433594\n104.23163604736328\n108.7791976928711\n106.8179931640625\n102.44072723388672\n115.43135070800781\n139.2732696533203\n139.0158233642578\n138.1632843017578\n138.2115478515625\n138.1384735107422\n138.2932891845703\n138.9574737548828\n139.6391143798828\n109.22981262207031\n138.13861083984375\n138.52072143554688\n141.20077514648438\n138.1566162109375\n138.2658233642578\n112.0488510131836\n122.05451202392578\n104.76752471923828\n105.54925537109375\n116.32136535644531\n114.98291015625\n115.0252685546875\n109.71922302246094\n138.56243896484375\n138.240234375\n108.24250030517578\n116.08833312988281\n122.34796905517578\n128.99740600585938\n104.02631378173828\n109.33892822265625\n144.04708862304688\n138.65554809570312\n103.62381744384766\n101.55809020996094\n100.7894287109375\n100.53002166748047\n139.10467529296875\n142.21331787109375\n101.19992065429688\n101.5877456665039\n101.5297622680664\n106.80104064941406\n146.82855224609375\n110.9051284790039\n138.32528686523438\n138.86386108398438\n139.53213500976562\n144.11465454101562\n138.14732360839844\n110.10496520996094\n139.54054260253906\n138.51193237304688\n92.18424224853516\n138.48538208007812\n139.36907958984375\n113.19622802734375\n100.60465240478516\n110.77095031738281\n138.3217315673828\n137.2557830810547\n378.99676513671875\n182.09226989746094\n127.60149383544922\n388.65399169921875\n101.15968322753906\n138.2177276611328\n138.76637268066406\n124.12654113769531\n113.13237762451172\n226.17205810546875\n108.02407836914062\n138.1654510498047\n109.92133331298828\n103.65464782714844\n138.3243865966797\n104.24225616455078\n125.79402923583984\n143.67184448242188\n104.2389144897461\n104.69508361816406\n106.66797637939453\n138.42112731933594\n132.14541625976562\n183.7557373046875\n139.9506072998047\n138.42869567871094\n125.55758666992188\n138.5436248779297\n110.943359375\n106.16215515136719\n104.38399505615234\n142.0206756591797\n101.24642944335938\n138.14089965820312\n108.81871032714844\n106.18631744384766\n100.41482543945312\n104.76484680175781\n140.35494995117188\n143.12692260742188\n123.53077697753906\n1174.29833984375\n103.87120056152344\n114.89098358154297\n113.6176986694336\n104.06500244140625\n9592.26953125\n123.6356201171875\n138.14599609375\n151.234375\n144.9730224609375\n138.7209930419922\n138.28134155273438\n146.43101501464844\n144.3245391845703\n160.499267578125\n138.7573699951172\n245.19866943359375\n200.4718017578125\n104.42719268798828\n102.56736755371094\n142.81723022460938\n138.2006378173828\n138.249267578125\n100.77327728271484\n93.99562072753906\n123.82850646972656\n103.26617431640625\n110.30374145507812\n118.67111206054688\n102.39424896240234\n138.17764282226562\n104.84988403320312\n109.57414245605469\n100.12364196777344\n138.13912963867188\n104.78034973144531\n100.97859191894531\n109.45512390136719\n138.15029907226562\n138.1435546875\n138.2454071044922\n647.7936401367188\n139.89512634277344\n109.41454315185547\n113.81166076660156\n138.6350555419922\n138.58438110351562\n103.28015899658203\n139.71401977539062\n104.94613647460938\n103.79739379882812\n113.95338439941406\n138.72756958007812\n119.03253173828125\n106.2965316772461\n100.15087127685547\n108.6072006225586\n108.76675415039062\n147.81167602539062\n140.74606323242188\n144.88385009765625\n124.0161361694336\n107.28252410888672\n138.1726531982422\n138.24249267578125\n138.15577697753906\n138.57789611816406\n100.88742065429688\n168.35198974609375\n103.9899673461914\n140.8463897705078\n139.25823974609375\n124.03886413574219\n100.1681137084961\n101.87287902832031\n138.50132751464844\n290.2644958496094\n141.02117919921875\n141.5286407470703\n138.22354125976562\n105.99485778808594\n124.91226196289062\n108.4213638305664\n102.85289764404297\n138.59469604492188\n103.84477233886719\n101.32438659667969\n138.25619506835938\n138.39720153808594\n100.99772644042969\n138.51324462890625\n93.53065490722656\n105.99494171142578\n107.04129791259766\n103.24195861816406\n88.2119140625\n87.01261138916016\n100.31092834472656\n101.004638671875\n105.37715911865234\n112.34564208984375\n100.61256408691406\n138.27403259277344\n111.46349334716797\n121.17157745361328\n86.64945983886719\n85.11365509033203\n93.25562286376953\n138.20469665527344\n101.95914459228516\n99.71694946289062\n100.19402313232422\n103.67622375488281\n103.74837493896484\n105.29105377197266\n110.05276489257812\n100.89140319824219\n105.51276397705078\n100.70450592041016\n92.60572814941406\n102.54041290283203\n107.32544708251953\n102.59706115722656\n89.95777893066406\n141.32118225097656\n86.77180480957031\n100.26710510253906\n100.66551971435547\n104.98900604248047\n100.12492370605469\n138.2032928466797\n101.90387725830078\n110.12232208251953\n105.3143081665039\n141.88983154296875\n101.95806121826172\n108.74634552001953\n85.65817260742188\n85.0816650390625\n101.23480987548828\n103.16545104980469\n100.88460540771484\n100.50650024414062\n90.05951690673828\n138.19064331054688\n124.20443725585938\n87.51649475097656\n100.15303039550781\n105.09188079833984\n138.15524291992188\n100.14433288574219\n110.34870147705078\n86.84966278076172\n106.55789947509766\n104.919921875\n124.48597717285156\n104.30323791503906\n87.80963897705078\n92.12711334228516\n108.81449890136719\n108.34473419189453\n84.22978210449219\n84.1959457397461\n83.36900329589844\n79.28375244140625\n77.67664337158203\n78.29945373535156\n77.43389129638672\n77.48477935791016\n160.74932861328125\n76.65983581542969\n76.91006469726562\n76.79888916015625\n77.32987976074219\n77.23241424560547\n77.58141326904297\n77.4672622680664\n76.87535858154297\n77.00357055664062\n78.20301818847656\n77.73676300048828\n77.3320083618164\n82.2680435180664\n77.80949401855469\n78.50474548339844\n78.16094207763672\n77.08544921875\n76.65396118164062\n77.60369110107422\n76.6800308227539\n77.01642608642578\n76.95339965820312\n76.57499694824219\n77.23773193359375\n77.02354431152344\n80.08912658691406\n78.48760223388672\n76.98648071289062\n77.34089660644531\n78.59330749511719\n77.74905395507812\n118.50360107421875\n77.75933837890625\n77.61991882324219\n78.45635223388672\n76.8975830078125\n78.37760162353516\n77.8077392578125\n77.01637268066406\n87.83059692382812\n77.60151672363281\n77.49263000488281\n78.01813507080078\n76.9944076538086\n77.93119049072266\n120.93940734863281\n78.75529479980469\n78.72984313964844\n77.52413177490234\n80.20772552490234\n78.37174224853516\n79.14224243164062\n77.7405776977539\n84.1959457397461\n77.58920288085938\n84.37275695800781\n84.21559143066406\n77.87891387939453\n77.8764419555664\n79.57904052734375\n78.38345336914062\n84.28114318847656\n78.76367950439453\n82.47360229492188\n82.32308959960938\n78.10114288330078\n77.08967590332031\n138.54977416992188\n84.25208282470703\n155.6998748779297\n97.61260986328125\n89.25886535644531\n89.83692169189453\n78.66421508789062\n82.05557250976562\n84.19940185546875\n140.48587036132812\n186.64321899414062\n157.17745971679688\n80.6258544921875\n79.30125427246094\n87.67718505859375\n138.23239135742188\n101.36125183105469\n142.96432495117188\n84.97772216796875\n138.1415252685547\n79.61913299560547\n82.01862335205078\n117.42524719238281\n76.4681396484375\n82.32527923583984\n84.2413330078125\n76.99899291992188\n76.39995574951172\n81.52644348144531\n114.70703125\n84.47840118408203\n146.38641357421875\n76.97095489501953\n79.11791229248047\n84.24577331542969\n78.74188232421875\n139.2926788330078\n79.95263671875\n78.09362030029297\n84.24334716796875\n138.37789916992188\n87.03265380859375\n114.79916381835938\n76.6996078491211\n78.65145874023438\n123.60084533691406\n104.01187133789062\n79.00524139404297\n80.8996810913086\n86.64115142822266\n84.2160873413086\n76.64387512207031\n80.12451934814453\n84.315185546875\n139.83836364746094\n84.24712371826172\n79.29307556152344\n79.07527923583984\n89.34788513183594\n138.14785766601562\n77.420166015625\n79.46617889404297\n105.37168884277344\n101.01988983154297\n89.43387603759766\n81.43977355957031\n82.42408752441406\n77.67007446289062\n79.91158294677734\n82.19037628173828\n80.08120727539062\n78.64347839355469\n77.08784484863281\n78.0699691772461\n77.66787719726562\n85.2590103149414\n104.17529296875\n85.17771911621094\n149.09188842773438\n77.68717956542969\n138.16802978515625\n84.72252655029297\n78.50462341308594\n84.20134735107422\n167.28958129882812\n85.79817962646484\n78.86092376708984\n79.05421447753906\n76.98846435546875\n77.5050277709961\n84.95382690429688\n84.51145935058594\n85.00394439697266\n80.4560546875\n77.59239196777344\n107.46865844726562\n84.25888061523438\n139.5652313232422\n137.88766479492188\n86.2657699584961\n139.67257690429688\n153.02597045898438\n146.9386749267578\n131.26806640625\n144.87493896484375\n299.4865417480469\n139.7495880126953\n138.57992553710938\n141.95181274414062\n9542.638671875\n193.32174682617188\n141.6095733642578\n158.22735595703125\n157.8621368408203\n151.3650360107422\n168.99398803710938\n140.812255859375\n4114.4755859375\n140.4001922607422\n147.1390380859375\n165.9713134765625\n142.0653839111328\n138.6419219970703\n119.36354064941406\n8300.392578125\n142.63037109375\n9375.0380859375\n144.81082153320312\n141.98898315429688\n141.0240478515625\n141.85003662109375\n138.99057006835938\n213.5583953857422\n120.29151916503906\n179.8900146484375\n138.71986389160156\n139.02130126953125\n140.26773071289062\n145.90989685058594\n144.8303985595703\n283.2796936035156\n8852.1962890625\n138.3087615966797\n169.60809326171875\n186.13836669921875\n144.32354736328125\n138.2496337890625\n154.82701110839844\n168.50367736816406\n159.7221221923828\n7969.72021484375\n142.04319763183594\n144.73428344726562\n138.15237426757812\n147.71836853027344\n138.14517211914062\n143.7833709716797\n138.2926788330078\n378.012451171875\n338.53033447265625\n142.22071838378906\n188.35748291015625\n1259.57470703125\n138.5310821533203\n139.6908416748047\n155.2303924560547\n172.56210327148438\n196.50448608398438\n9392.9501953125\n228.57058715820312\n140.53375244140625\n150.74554443359375\n172.2469940185547\n184.94293212890625\n175.78756713867188\n140.99774169921875\n139.57144165039062\n140.94818115234375\n131.5008544921875\n114.70254516601562\n143.25814819335938\n138.3376922607422\n259.83099365234375\n303.6153869628906\n144.01058959960938\n197.50311279296875\n8750.76171875\n175.31454467773438\n9042.671875\n139.7306365966797\n172.65325927734375\n158.4794921875\n218.23977661132812\n142.1581268310547\n167.45086669921875\n143.61550903320312\n138.66197204589844\n138.34640502929688\n140.14305114746094\n138.9364776611328\n141.1790771484375\n206.6788787841797\n138.54669189453125\n139.1255340576172\n159.01553344726562\n143.7413330078125\n140.1911163330078\n145.65696716308594\n139.5675811767578\n146.87301635742188\n154.0835723876953\n9624.6748046875\n138.80311584472656\n138.15036010742188\n139.8462371826172\n138.6008758544922\n143.05137634277344\n149.8837127685547\n139.88478088378906\n139.41905212402344\n139.84275817871094\n153.1545867919922\n138.69454956054688\n183.14732360839844\n140.27230834960938\n9648.755859375\n168.84532165527344\n141.19921875\n138.4871063232422\n141.27560424804688\n319.48382568359375\n669.4497680664062\n141.61988830566406\n146.32984924316406\n152.5666961669922\n142.08602905273438\n159.38925170898438\n281.8335876464844\n139.1084442138672\n246.20452880859375\n138.3079376220703\n138.66586303710938\n139.2769775390625\n193.29910278320312\n138.7967529296875\n138.3570098876953\n138.32562255859375\n140.09661865234375\n280.09197998046875\n140.39022827148438\n150.69345092773438\n151.3791961669922\n138.46983337402344\n108.47674560546875\n147.7694091796875\n7362.6943359375\n138.14312744140625\n142.01123046875\n192.9137725830078\n138.2001495361328\n152.2855224609375\n169.31202697753906\n146.01466369628906\n163.1387481689453\n142.6287384033203\n155.28497314453125\n142.35731506347656\n144.35772705078125\n138.2083740234375\n172.0625\n138.92774963378906\n148.96145629882812\n160.80657958984375\n141.62911987304688\n152.12527465820312\n198.89215087890625\n139.46481323242188\n138.16148376464844\n110.32392120361328\n352.4618835449219\n138.82191467285156\n107.09009552001953\n568.345703125\n138.8606414794922\n7792.9873046875\n174.71890258789062\n260.154296875\n138.7138214111328\n251.367431640625\n7103.2724609375\n139.99331665039062\n140.9042510986328\n141.7918243408203\n138.81588745117188\n154.76174926757812\n138.29904174804688\n140.74986267089844\n138.6795196533203\n138.5554656982422\n140.64476013183594\n158.1012420654297\n139.26441955566406\n214.10714721679688\n141.6009521484375\n138.8326416015625\n138.94818115234375\n139.647216796875\n138.33580017089844\n138.730224609375\n139.22735595703125\n100.21694946289062\n140.80810546875\n160.28271484375\n150.29989624023438\n155.84671020507812\n152.12722778320312\n105.8634262084961\n107.28360748291016\n107.1343765258789\n9577.70703125\n138.68690490722656\n140.28826904296875\n109.15786743164062\n143.36282348632812\n145.57208251953125\n245.21038818359375\n135.02685546875\n138.2970428466797\n7728.66259765625\n9602.2998046875\n110.4677734375\n128.29510498046875\n140.562255859375\n138.56736755371094\n4248.71728515625\n138.18887329101562\n143.06622314453125\n138.1760711669922\n148.9459991455078\n155.83688354492188\n826.3719482421875\n140.96621704101562\n138.91647338867188\n138.29742431640625\n140.46597290039062\n149.8961639404297\n138.8116455078125\n139.98846435546875\n138.57949829101562\n154.04632568359375\n143.5452880859375\n142.58084106445312\n7976.5888671875\n144.1325225830078\n139.22613525390625\n138.19354248046875\n141.8675994873047\n109.5177230834961\n141.79542541503906\n147.3379669189453\n142.8612060546875\n138.2065887451172\n140.4397735595703\n550.8815307617188\n138.31649780273438\n5530.22265625\n164.6127471923828\n142.46067810058594\n137.80685424804688\n140.5431365966797\n141.0546417236328\n155.85879516601562\n138.57962036132812\n183.19081115722656\n125.15110778808594\n138.24949645996094\n138.1566162109375\n138.27166748046875\n138.2087860107422\n243.49026489257812\n150.42962646484375\n9743.310546875\n7339.69873046875\n149.6272735595703\n100.78246307373047\n590.8076782226562\n161.48216247558594\n140.3663787841797\n138.50489807128906\n138.3795623779297\n116.31783294677734\n107.3281021118164\n139.76922607421875\n138.27711486816406\n146.14842224121094\n167.7281494140625\n148.55728149414062\n138.1921844482422\n145.1349639892578\n147.552978515625\n149.31112670898438\n149.689453125\n139.60792541503906\n108.53749084472656\n154.99464416503906\n171.1642303466797\n143.6471710205078\n2424.05029296875\n136.02639770507812\n140.44151306152344\n119.6216049194336\n121.52574920654297\n138.98257446289062\n8632.9345703125\n168.23924255371094\n139.81785583496094\n8619.576171875\n105.81542205810547\n112.53395080566406\n163.34141540527344\n139.23208618164062\n138.425537109375\n8734.6943359375\n184.61412048339844\n138.8803253173828\n138.16812133789062\n140.14060974121094\n139.390625\n144.23329162597656\n154.84718322753906\n166.47872924804688\n141.51309204101562\n148.9847869873047\n144.20187377929688\n138.1465301513672\n146.0052490234375\n227.40887451171875\n142.15501403808594\n138.838623046875\n139.68617248535156\n142.30267333984375\n139.3419952392578\n174.8414306640625\n474.88116455078125\n204.17691040039062\n159.98939514160156\n143.7732391357422\n143.1627197265625\n143.45765686035156\n147.17300415039062\n109.52484130859375\n6091.92822265625\n138.2097625732422\n138.64352416992188\n142.0914764404297\n146.77566528320312\n138.15383911132812\n8316.0185546875\n142.9099884033203\n140.23997497558594\n142.4590606689453\n145.41160583496094\n255.36766052246094\n161.0284423828125\n2951.597900390625\n140.941650390625\n120.84237670898438\n138.83233642578125\n451.14788818359375\n159.70852661132812\n123.68872833251953\n138.92205810546875\n190.05999755859375\n144.3243865966797\n107.02143096923828\n140.94259643554688\n149.1020965576172\n145.9397430419922\n2148.618896484375\n139.29312133789062\n142.54974365234375\n125.81163024902344\n143.1884307861328\n150.55335998535156\n115.92430114746094\n139.52049255371094\n141.87539672851562\n141.00054931640625\n138.3988800048828\n155.84768676757812\n140.92556762695312\n145.184326171875\n139.30531311035156\n139.68472290039062\n112.8340072631836\n139.36366271972656\n141.34817504882812\n139.2515411376953\n122.89525604248047\n116.22904968261719\n110.55430603027344\n138.1744842529297\n138.20912170410156\n376.77459716796875\n141.3939971923828\n152.12818908691406\n139.66387939453125\n142.98455810546875\n138.26705932617188\n139.10604858398438\n197.25564575195312\n142.15182495117188\n144.1009063720703\n166.47793579101562\n153.3916015625\n139.23123168945312\n139.6377716064453\n191.4126434326172\n141.65170288085938\n145.7857666015625\n138.59442138671875\n176.11651611328125\n151.2943115234375\n139.1227264404297\n144.4725341796875\n139.89088439941406\n9714.6494140625\n143.74093627929688\n150.48764038085938\n163.09423828125\n139.1393280029297\n143.32981872558594\n139.94822692871094\n138.1850128173828\n124.49150848388672\n151.010009765625\n167.70620727539062\n113.92738342285156\n108.39883422851562\n123.08421325683594\n138.52914428710938\n138.9949951171875\n1721"
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-16dd205d4da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbuff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminheap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminheap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(len(rbuff)-512):\n",
    "    print(i, end='\\r')\n",
    "    if type(rbuff.minheap.heap[i][0]) != type(2.2) or 1:\n",
    "        print(rbuff.minheap.heap[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n<class 'float'>\n"
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-8ecf31869105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbuff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminheap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(len(rbuff)):\n",
    "    print(type(rbuff.minheap.heap[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(-9410.3427734375,\n  tensor([[ 0.3903,  0.0984,  0.6862, -0.2911, -2.2004, -2.1916,  0.0000,  0.0000]]),\n  tensor([[-0.3205, -0.4049]]),\n  tensor([[-100.]]),\n  tensor([[ 0.3984,  0.0981,  0.7741, -0.0522, -2.2540, -1.1055,  0.0000,  0.0000]]),\n  tensor([[1.]])),\n (-9410.3427734375,\n  tensor([[ 0.3903,  0.0984,  0.6862, -0.2911, -2.2004, -2.1916,  0.0000,  0.0000]]),\n  tensor([[-0.3205, -0.4049]]),\n  tensor([[-100.]]),\n  tensor([[ 0.3984,  0.0981,  0.7741, -0.0522, -2.2540, -1.1055,  0.0000,  0.0000]]),\n  tensor([[1.]])),\n (-1.0529377460479736,\n  tensor([[ 0.0505,  1.4812,  0.1638, -0.0376,  0.0190,  0.0817,  0.0000,  0.0000]]),\n  tensor([[-0.9213, -0.4799]]),\n  tensor([[-1.0936]]),\n  tensor([[ 0.0522,  1.4797,  0.1638, -0.0652,  0.0231,  0.0817,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-738.4655151367188,\n  tensor([[-0.0057,  1.4396, -0.1884,  0.4452,  0.0037,  0.0176,  0.0000,  0.0000]]),\n  tensor([[ 0.7040, -0.6778]]),\n  tensor([[-3.7636]]),\n  tensor([[-0.0075,  1.4501, -0.1907,  0.4680,  0.0060,  0.0467,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.8486300110816956,\n  tensor([[ 0.0418,  1.4829,  0.1813,  0.0439, -0.0029,  0.0664,  0.0000,  0.0000]]),\n  tensor([[-0.2403, -0.6412]]),\n  tensor([[1.1253]]),\n  tensor([[0.0436, 1.4833, 0.1749, 0.0173, 0.0017, 0.0917, 0.0000, 0.0000]]),\n  tensor([[0.]])),\n (-1.024644136428833,\n  tensor([[ 0.1258,  0.7550,  0.3968, -0.7511, -0.3512, -0.2933,  0.0000,  0.0000]]),\n  tensor([[0.7862, 0.5870]]),\n  tensor([[-0.6495]]),\n  tensor([[ 0.1297,  0.7383,  0.4131, -0.7461, -0.3673, -0.3211,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.9848355650901794,\n  tensor([[ 0.0556,  1.4750,  0.1638, -0.1186,  0.0313,  0.0817,  0.0000,  0.0000]]),\n  tensor([[0.2180, 0.3015]]),\n  tensor([[-0.2917]]),\n  tensor([[ 0.0573,  1.4725,  0.1688, -0.1098,  0.0357,  0.0881,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-710.1622924804688,\n  tensor([[-0.0285,  0.0992, -0.0994, -1.1313,  0.0121, -0.0735,  0.0000,  0.0000]]),\n  tensor([[ 0.5494, -0.8961]]),\n  tensor([[4.4746]]),\n  tensor([[-0.0297,  0.0743, -0.1220, -1.1069,  0.0101, -0.0413,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-711.3705444335938,\n  tensor([[-0.0383,  1.5595, -0.1907,  0.2048,  0.0105,  0.0072,  0.0000,  0.0000]]),\n  tensor([[-0.7983,  0.3192]]),\n  tensor([[1.4475]]),\n  tensor([[-0.0402,  1.5635, -0.1907,  0.1781,  0.0109,  0.0072,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.8485422730445862,\n  tensor([[-0.0219,  0.2485, -0.0926, -1.0465,  0.0410, -0.1043,  0.0000,  0.0000]]),\n  tensor([[-0.0649, -0.2241]]),\n  tensor([[0.2593]]),\n  tensor([[-0.0229,  0.2243, -0.0926, -1.0732,  0.0358, -0.1043,  0.0000,  0.0000]]),\n  tensor([[0.]]))]"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "rbuff.minheap.heap[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[ 0.6434, -0.3702]]),\n  tensor([[3.9034]]),\n  tensor([[-0.0140,  0.4453, -0.0687, -0.9272,  0.0980, -0.0933,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.22102661430835724,\n  tensor([[ 0.0133,  1.4321,  0.2022,  0.1684, -0.0069,  0.0168,  0.0000,  0.0000]]),\n  tensor([[0.9590, 0.9289]]),\n  tensor([[-3.1459]]),\n  tensor([[ 0.0154,  1.4364,  0.2131,  0.1908, -0.0078, -0.0195,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.21037587523460388,\n  tensor([[-0.0191,  1.5021, -0.1917,  0.3286,  0.0188,  0.0054,  0.0000,  0.0000]]),\n  tensor([[ 0.1198, -0.4621]]),\n  tensor([[-1.5380]]),\n  tensor([[-0.0210,  1.5097, -0.1911,  0.3356,  0.0191,  0.0070,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.4519810378551483,\n  tensor([[ 0.0539,  1.4777,  0.1638, -0.0919,  0.0272,  0.0817,  0.0000,  0.0000]]),\n  tensor([[-0.9598,  0.3461]]),\n  tensor([[-1.5870]]),\n  tensor([[ 0.0556,  1.4750,  0.1638, -0.1186,  0.0313,  0.0817,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.9456573724746704,\n  tensor([[ 0.1240,  1.1927,  0.0844, -0.4895,  0.2322,  0.1730,  0.0000,  0.0000]]),\n  tensor([[-0.4225, -0.5106]]),\n  tensor([[-2.4393]]),\n  tensor([[ 0.1249,  1.1811,  0.0801, -0.5167,  0.2417,  0.1909,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.8894459009170532,\n  tensor([[-0.0105,  0.5920, -0.0431, -0.8599,  0.1146, -0.0102,  0.0000,  0.0000]]),\n  tensor([[-0.5516,  0.0189]]),\n  tensor([[-0.6188]]),\n  tensor([[-0.0110,  0.5721, -0.0431, -0.8865,  0.1141, -0.0102,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.0891706943511963,\n  tensor([[-0.0075,  1.4501, -0.1907,  0.4680,  0.0060,  0.0467,  0.0000,  0.0000]]),\n  tensor([[-0.5995, -0.6479]]),\n  tensor([[0.7806]]),\n  tensor([[-0.0095,  1.4601, -0.1974,  0.4416,  0.0097,  0.0736,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.1080474853515625,\n  tensor([[-0.0127,  0.4875, -0.0460, -0.9408,  0.1068, -0.0815,  0.0000,  0.0000]]),\n  tensor([[ 0.7290, -0.4272]]),\n  tensor([[1.8915]]),\n  tensor([[-0.0133,  0.4662, -0.0493, -0.9444,  0.1027, -0.0811,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.08342235535383224,\n  tensor([[-0.0156,  0.4033, -0.0752, -0.9471,  0.0872, -0.1169,  0.0000,  0.0000]]),\n  tensor([[0.2312, 0.5487]]),\n  tensor([[3.3059]]),\n  tensor([[-0.0164,  0.3821, -0.0712, -0.9403,  0.0804, -0.1376,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.22621390223503113,\n  tensor([[ 0.0451,  1.1222,  0.2068, -0.4589, -0.1310, -0.1175,  0.0000,  0.0000]]),\n  tensor([[0.4737, 0.1535]]),\n  tensor([[-0.4443]]),\n  tensor([[ 0.0473,  1.1119,  0.2217, -0.4597, -0.1364, -0.1080,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.07504884153604507,\n  tensor([[ 0.1271,  1.0856, -0.0185, -0.5181,  0.3063,  0.1585,  0.0000,  0.0000]]),\n  tensor([[ 0.4309, -0.5481]]),\n  tensor([[1.4088]]),\n  tensor([[ 0.1267,  1.0744, -0.0450, -0.5022,  0.3153,  0.1805,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.3408479690551758,\n  tensor([[ 0.1082,  1.3089,  0.1650, -0.4264,  0.1666,  0.0914,  0.0000,  0.0000]]),\n  tensor([[0.9651, 0.5054]]),\n  tensor([[0.5659]]),\n  tensor([[ 0.1099,  1.2993,  0.1637, -0.4237,  0.1702,  0.0718,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.5985360145568848,\n  tensor([[ 0.1261,  1.0632, -0.0712, -0.4969,  0.3239,  0.1707,  0.0000,  0.0000]]),\n  tensor([[-0.8993,  0.4094]]),\n  tensor([[-2.3222]]),\n  tensor([[ 0.1254,  1.0515, -0.0712, -0.5236,  0.3324,  0.1707,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.6310457587242126,\n  tensor([[ 0.0702,  0.9942,  0.2545, -0.5926, -0.1950, -0.1656,  0.0000,  0.0000]]),\n  tensor([[0.8265, 0.9191]]),\n  tensor([[0.1499]]),\n  tensor([[ 0.0730,  0.9814,  0.2929, -0.5726, -0.2049, -0.1973,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.6214376091957092,\n  tensor([[-0.0267,  1.5287, -0.1866,  0.2555,  0.0175, -0.0107,  0.0000,  0.0000]]),\n  tensor([[0.9580, 0.5258]]),\n  tensor([[-1.0031]]),\n  tensor([[-0.0286,  1.5345, -0.1922,  0.2551,  0.0156, -0.0380,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.7493414878845215,\n  tensor([[ 0.0758,  0.9688,  0.2850, -0.5597, -0.2130, -0.1609,  0.0000,  0.0000]]),\n  tensor([[-0.3860,  0.6958]]),\n  tensor([[-2.4227]]),\n  tensor([[ 0.0786,  0.9556,  0.2916, -0.5872, -0.2223, -0.1879,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.1963287591934204,\n  tensor([[ 0.1219,  1.0176, -0.1421, -0.5026,  0.3560,  0.1547,  0.0000,  0.0000]]),\n  tensor([[ 0.0399, -0.5136]]),\n  tensor([[-1.2450]]),\n  tensor([[ 0.1203,  1.0062, -0.1682, -0.5088,  0.3646,  0.1728,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.44051408767700195,\n  tensor([[ 0.1113,  1.2903,  0.1444, -0.4001,  0.1734,  0.0658,  0.0000,  0.0000]]),\n  tensor([[-0.3712,  0.3388]]),\n  tensor([[-1.9036]]),\n  tensor([[ 0.1128,  1.2808,  0.1444, -0.4267,  0.1767,  0.0658,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.3455427885055542,\n  tensor([[-0.0325,  1.5449, -0.1984,  0.2363,  0.0116, -0.0434,  0.0000,  0.0000]]),\n  tensor([[0.5217, 0.4523]]),\n  tensor([[0.2412]]),\n  tensor([[-0.0344,  1.5502, -0.1864,  0.2352,  0.0100, -0.0311,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.3207896947860718,\n  tensor([[ 0.1128,  1.2808,  0.1444, -0.4267,  0.1767,  0.0658,  0.0000,  0.0000]]),\n  tensor([[ 0.4663, -0.6811]]),\n  tensor([[2.3729]]),\n  tensor([[ 0.1142,  1.2716,  0.1369, -0.4060,  0.1816,  0.0976,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.5748574733734131,\n  tensor([[ 0.1170,  0.9816, -0.1748, -0.5638,  0.3848,  0.2022,  0.0000,  0.0000]]),\n  tensor([[ 0.6675, -0.6679]]),\n  tensor([[-2.2110]]),\n  tensor([[ 0.1151,  0.9687, -0.2031, -0.5767,  0.3961,  0.2247,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.0223157405853271,\n  tensor([[ 0.1099,  1.2993,  0.1637, -0.4237,  0.1702,  0.0718,  0.0000,  0.0000]]),\n  tensor([[0.5940, 0.1653]]),\n  tensor([[3.2004]]),\n  tensor([[ 0.1113,  1.2903,  0.1444, -0.4001,  0.1734,  0.0658,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.1625384092330933,\n  tensor([[ 0.0712,  1.4454,  0.1624, -0.1819,  0.0552,  0.0621,  0.0000,  0.0000]]),\n  tensor([[ 0.3932, -0.7692]]),\n  tensor([[0.1312]]),\n  tensor([[ 0.0729,  1.4415,  0.1636, -0.1746,  0.0600,  0.0961,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.7855241298675537,\n  tensor([[ 0.1142,  1.2716,  0.1369, -0.4060,  0.1816,  0.0976,  0.0000,  0.0000]]),\n  tensor([[-0.6327,  0.4774]]),\n  tensor([[-2.0676]]),\n  tensor([[ 0.1156,  1.2619,  0.1369, -0.4327,  0.1865,  0.0976,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.5549790859222412,\n  tensor([[ 0.1083,  0.9277, -0.2541, -0.6230,  0.4332,  0.2605,  0.0000,  0.0000]]),\n  tensor([[0.9699, 0.7592]]),\n  tensor([[1.4267]]),\n  tensor([[ 0.1057,  0.9144, -0.2751, -0.5971,  0.4448,  0.2318,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.4275486469268799,\n  tensor([[ 0.1030,  0.9004, -0.2844, -0.6272,  0.4587,  0.2768,  0.0000,  0.0000]]),\n  tensor([[ 0.9266, -0.6417]]),\n  tensor([[-2.3428]]),\n  tensor([[ 0.0999,  0.8863, -0.3249, -0.6296,  0.4735,  0.2966,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.1917167603969574,\n  tensor([[ 0.1156,  1.2619,  0.1369, -0.4327,  0.1865,  0.0976,  0.0000,  0.0000]]),\n  tensor([[0.8957, 0.9746]]),\n  tensor([[3.0027]]),\n  tensor([[ 0.1171,  1.2529,  0.1417, -0.4022,  0.1896,  0.0620,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.0259047746658325,\n  tensor([[ 0.0583,  1.0566,  0.2251, -0.4942, -0.1564, -0.0906,  0.0000,  0.0000]]),\n  tensor([[-0.8318,  0.8970]]),\n  tensor([[-2.4791]]),\n  tensor([[ 0.0606,  1.0449,  0.2357, -0.5217, -0.1631, -0.1334,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.3411247730255127,\n  tensor([[ 0.0608,  1.4669,  0.1723, -0.1143,  0.0389,  0.0121,  0.0000,  0.0000]]),\n  tensor([[-0.5501,  0.1627]]),\n  tensor([[-1.3363]]),\n  tensor([[ 0.0626,  1.4637,  0.1723, -0.1410,  0.0395,  0.0121,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.6985042095184326,\n  tensor([[ 0.0969,  0.8784,  0.3352, -0.5858, -0.2631, -0.1350,  0.0000,  0.0000]]),\n  tensor([[0.4386, 0.8475]]),\n  tensor([[-1.8810]]),\n  tensor([[ 0.1004,  0.8650,  0.3577, -0.5969, -0.2714, -0.1677,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.5619182586669922,\n  tensor([[-9.7138e-03,  6.3010e-01, -4.1826e-02, -8.2877e-01,  1.1669e-01,\n           -3.8182e-06,  0.0000e+00,  0.0000e+00]]),\n  tensor([[0.0621, 0.7834]]),\n  tensor([[1.4846]]),\n  tensor([[-0.0101,  0.6114, -0.0379, -0.8326,  0.1151, -0.0312,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.6725162267684937,\n  tensor([[ 0.1171,  1.2529,  0.1417, -0.4022,  0.1896,  0.0620,  0.0000,  0.0000]]),\n  tensor([[0.1854, 0.1746]]),\n  tensor([[0.2916]]),\n  tensor([[ 0.1185,  1.2438,  0.1422, -0.4028,  0.1931,  0.0696,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.630208134651184,\n  tensor([[ 0.0788,  0.7993, -0.3739, -0.6878,  0.5571,  0.2708,  0.0000,  0.0000]]),\n  tensor([[-0.2862, -0.9116]]),\n  tensor([[-3.0391]]),\n  tensor([[ 0.0751,  0.7833, -0.3821, -0.7183,  0.5728,  0.3139,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.3545580804347992,\n  tensor([[ 0.1039,  0.8510,  0.3651, -0.6246, -0.2814, -0.1982,  0.0000,  0.0000]]),\n  tensor([[-0.4017,  0.0244]]),\n  tensor([[-1.9004]]),\n  tensor([[ 0.1075,  0.8364,  0.3651, -0.6513, -0.2913, -0.1982,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.25351929664611816,\n  tensor([[ 0.0824,  0.8147, -0.3739, -0.6611,  0.5436,  0.2708,  0.0000,  0.0000]]),\n  tensor([[-0.7860, -0.0969]]),\n  tensor([[-2.1215]]),\n  tensor([[ 0.0788,  0.7993, -0.3739, -0.6878,  0.5571,  0.2708,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.3566409349441528,\n  tensor([[ 0.2323,  0.3359,  0.6445, -0.8970, -0.6158, -0.0796,  0.0000,  0.0000]]),\n  tensor([[-0.0675,  0.9570]]),\n  tensor([[-2.1664]]),\n  tensor([[ 0.2388,  0.3151,  0.6516, -0.9271, -0.6216, -0.1174,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.8976883292198181,\n  tensor([[ 0.0175,  1.4411,  0.2077,  0.2112, -0.0091, -0.0254,  0.0000,  0.0000]]),\n  tensor([[ 0.1240, -0.3214]]),\n  tensor([[-1.3346]]),\n  tensor([[ 0.0196,  1.4460,  0.2132,  0.2140, -0.0101, -0.0204,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.0576467514038086,\n  tensor([[ 0.1110,  0.8212,  0.3651, -0.6779, -0.3012, -0.1982,  0.0000,  0.0000]]),\n  tensor([[-0.6871,  0.7510]]),\n  tensor([[-2.4510]]),\n  tensor([[ 0.1147,  0.8053,  0.3719, -0.7061, -0.3126, -0.2278,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.07991496473550797,\n  tensor([[ 0.1075,  0.8364,  0.3651, -0.6513, -0.2913, -0.1982,  0.0000,  0.0000]]),\n  tensor([[-0.5341, -0.0652]]),\n  tensor([[-1.8673]]),\n  tensor([[ 0.1110,  0.8212,  0.3651, -0.6779, -0.3012, -0.1982,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.6492688059806824,\n  tensor([[ 0.0626,  1.4637,  0.1723, -0.1410,  0.0395,  0.0121,  0.0000,  0.0000]]),\n  tensor([[ 0.5878, -0.7870]]),\n  tensor([[-0.9016]]),\n  tensor([[ 0.0644,  1.4605,  0.1789, -0.1443,  0.0420,  0.0490,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.37968242168426514,\n  tensor([[ 0.2589,  0.2504,  0.6898, -0.9776, -0.6428, -0.1529,  0.0000,  0.0000]]),\n  tensor([[ 0.3361, -0.9895]]),\n  tensor([[-0.7105]]),\n  tensor([[ 0.2659,  0.2285,  0.7047, -0.9780, -0.6483, -0.1099,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.4158475399017334,\n  tensor([[ 0.2732,  0.2068,  0.7366, -0.9631, -0.6544, -0.1217,  0.0000,  0.0000]]),\n  tensor([[0.4130, 0.2526]]),\n  tensor([[-2.8940]]),\n  tensor([[ 0.2810,  0.1852,  0.7814, -0.9627, -0.6599, -0.1112,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.7406550645828247,\n  tensor([[ 0.1183,  0.7889,  0.3719, -0.7328, -0.3240, -0.2278,  0.0000,  0.0000]]),\n  tensor([[-0.8928,  0.8042]]),\n  tensor([[-2.3912]]),\n  tensor([[ 0.1219,  0.7719,  0.3778, -0.7604, -0.3366, -0.2524,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.5641014575958252,\n  tensor([[ 0.2888,  0.1630,  0.7814, -0.9894, -0.6655, -0.1112,  0.0000,  0.0000]]),\n  tensor([[-0.6584,  0.2153]]),\n  tensor([[-2.3003]]),\n  tensor([[ 0.2965,  0.1402,  0.7814, -1.0160, -0.6710, -0.1112,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.9358636736869812,\n  tensor([[ 0.1219,  0.7719,  0.3778, -0.7604, -0.3366, -0.2524,  0.0000,  0.0000]]),\n  tensor([[0.5625, 0.9511]]),\n  tensor([[-0.1618]]),\n  tensor([[ 0.1258,  0.7550,  0.3968, -0.7511, -0.3512, -0.2933,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.4443393349647522,\n  tensor([[ 0.0629,  1.0329,  0.2349, -0.5338, -0.1699, -0.1376,  0.0000,  0.0000]]),\n  tensor([[-0.6487,  0.8514]]),\n  tensor([[-2.4651]]),\n  tensor([[ 0.0652,  1.0203,  0.2432, -0.5611, -0.1785, -0.1708,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.43214714527130127,\n  tensor([[ 0.1593,  0.6153,  0.4628, -0.8054, -0.4756, -0.3131,  0.0000,  0.0000]]),\n  tensor([[-0.2680, -0.7913]]),\n  tensor([[-1.5204]]),\n  tensor([[ 0.1637,  0.5968,  0.4556, -0.8298, -0.4896, -0.2798,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (1.0362564325332642,\n  tensor([[ 0.0661,  1.4574,  0.1741, -0.1358,  0.0443,  0.0465,  0.0000,  0.0000]]),\n  tensor([[-0.6050, -0.5129]]),\n  tensor([[-1.4300]]),\n  tensor([[ 0.0679,  1.4538,  0.1699, -0.1628,  0.0474,  0.0630,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.6407383680343628,\n  tensor([[ 0.1338,  0.7216,  0.4264, -0.7470, -0.3834, -0.3225,  0.0000,  0.0000]]),\n  tensor([[ 0.4458, -0.3136]]),\n  tensor([[-0.7542]]),\n  tensor([[ 0.1380,  0.7048,  0.4268, -0.7519, -0.4001, -0.3340,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.9218187928199768,\n  tensor([[ 0.0378,  1.1651,  0.1807, -0.4704, -0.1047, -0.1591,  0.0000,  0.0000]]),\n  tensor([[-0.6556,  0.4954]]),\n  tensor([[-2.2665]]),\n  tensor([[ 0.0395,  1.1539,  0.1805, -0.4981, -0.1126, -0.1591,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.25318649411201477,\n  tensor([[ 0.3300,  0.1215,  0.6052, -0.0666, -1.2221, -2.2210,  1.0000,  0.0000]]),\n  tensor([[-0.3152, -0.7688]]),\n  tensor([[-21.2227]]),\n  tensor([[ 0.3357,  0.1212,  0.6001, -0.0877, -1.3312, -2.1786,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (2.997385025024414,\n  tensor([[ 0.1231,  1.2037,  0.0844, -0.4629,  0.2235,  0.1730,  0.0000,  0.0000]]),\n  tensor([[-0.8619, -0.0633]]),\n  tensor([[-2.4084]]),\n  tensor([[ 0.1240,  1.1927,  0.0844, -0.4895,  0.2322,  0.1730,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.6773251891136169,\n  tensor([[ 0.0984,  1.3590,  0.1485, -0.2899,  0.1327,  0.1377,  0.0000,  0.0000]]),\n  tensor([[-0.5126, -0.3973]]),\n  tensor([[-2.3857]]),\n  tensor([[ 0.1000,  1.3518,  0.1485, -0.3166,  0.1396,  0.1377,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.387265682220459,\n  tensor([[ 0.3481,  0.1192,  0.6383, -0.1302, -1.5489, -2.1822,  0.0000,  0.0000]]),\n  tensor([[-0.7697, -0.7728]]),\n  tensor([[-11.6545]]),\n  tensor([[ 0.3545,  0.1174,  0.6367, -0.1531, -1.6564, -2.1511,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.4312398433685303,\n  tensor([[ 0.3418,  0.1205,  0.6406, -0.1033, -1.4399, -2.1762,  0.0000,  0.0000]]),\n  tensor([[-0.0149, -0.3742]]),\n  tensor([[-11.7148]]),\n  tensor([[ 0.3481,  0.1192,  0.6383, -0.1302, -1.5489, -2.1822,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.3348102867603302,\n  tensor([[ 0.3610,  0.1149,  0.6352, -0.1797, -1.7638, -2.1494,  0.0000,  0.0000]]),\n  tensor([[0.3331, 0.0091]]),\n  tensor([[-17.8611]]),\n  tensor([[ 0.3682,  0.1119,  0.6927, -0.2071, -1.8716, -2.1575,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.37011298537254333,\n  tensor([[ 0.3755,  0.1081,  0.6891, -0.2386, -1.9813, -2.1952,  0.0000,  0.0000]]),\n  tensor([[-0.4692,  0.4545]]),\n  tensor([[-12.3150]]),\n  tensor([[ 0.3829,  0.1036,  0.6876, -0.2649, -2.0909, -2.1934,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.29962319135665894,\n  tensor([[ 0.1249,  1.1811,  0.0801, -0.5167,  0.2417,  0.1909,  0.0000,  0.0000]]),\n  tensor([[ 0.6778, -0.5766]]),\n  tensor([[1.2800]]),\n  tensor([[ 0.1255,  1.1698,  0.0447, -0.5064,  0.2520,  0.2062,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.9942751526832581,\n  tensor([[ 0.1637,  0.5968,  0.4556, -0.8298, -0.4896, -0.2798,  0.0000,  0.0000]]),\n  tensor([[-0.1776, -0.4184]]),\n  tensor([[-2.0215]]),\n  tensor([[ 0.1682,  0.5776,  0.4556, -0.8565, -0.5036, -0.2798,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.3920590877532959,\n  tensor([[-0.0019,  1.4199, -0.1880,  0.3984,  0.0022,  0.0426,  0.0000,  0.0000]]),\n  tensor([[0.9134, 0.5751]]),\n  tensor([[-4.5400]]),\n  tensor([[-0.0038,  1.4296, -0.1930,  0.4313,  0.0028,  0.0127,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.05024990811944008,\n  tensor([[-0.0038,  1.4296, -0.1930,  0.4313,  0.0028,  0.0127,  0.0000,  0.0000]]),\n  tensor([[ 0.5443, -0.1714]]),\n  tensor([[-2.4088]]),\n  tensor([[-0.0057,  1.4396, -0.1884,  0.4452,  0.0037,  0.0176,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.45283061265945435,\n  tensor([[ 0.0196,  1.4460,  0.2132,  0.2140, -0.0101, -0.0204,  0.0000,  0.0000]]),\n  tensor([[0.0986, 0.3162]]),\n  tensor([[-1.3508]]),\n  tensor([[ 0.0217,  1.4509,  0.2174,  0.2183, -0.0110, -0.0166,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-704.8316040039062,\n  tensor([[-0.0364,  1.5549, -0.1948,  0.2087,  0.0101,  0.0027,  0.0000,  0.0000]]),\n  tensor([[ 0.1117, -0.0654]]),\n  tensor([[-0.0946]]),\n  tensor([[-0.0383,  1.5595, -0.1907,  0.2048,  0.0105,  0.0072,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.00582762248814106,\n  tensor([[-0.0095,  1.4601, -0.1974,  0.4416,  0.0097,  0.0736,  0.0000,  0.0000]]),\n  tensor([[-0.9972,  0.6307]]),\n  tensor([[1.5235]]),\n  tensor([[-0.0114,  1.4694, -0.1907,  0.4150,  0.0120,  0.0463,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.37430936098098755,\n  tensor([[-0.0173,  0.3609, -0.0808, -0.9438,  0.0731, -0.1442,  0.0000,  0.0000]]),\n  tensor([[-0.1132,  0.2837]]),\n  tensor([[0.2406]]),\n  tensor([[-0.0182,  0.3390, -0.0808, -0.9705,  0.0659, -0.1442,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.026913989335298538,\n  tensor([[-0.0133,  1.4782, -0.1965,  0.3887,  0.0155,  0.0697,  0.0000,  0.0000]]),\n  tensor([[0.3018, 0.9582]]),\n  tensor([[-0.6971]]),\n  tensor([[-0.0153,  1.4867, -0.1980,  0.3818,  0.0170,  0.0304,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.8163566589355469,\n  tensor([[ 0.1682,  0.5776,  0.4556, -0.8565, -0.5036, -0.2798,  0.0000,  0.0000]]),\n  tensor([[ 0.4005, -0.9526]]),\n  tensor([[0.4029]]),\n  tensor([[ 0.1726,  0.5585,  0.4565, -0.8544, -0.5156, -0.2410,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.02075899951159954,\n  tensor([[-0.0153,  1.4867, -0.1980,  0.3818,  0.0170,  0.0304,  0.0000,  0.0000]]),\n  tensor([[-0.1989,  0.2147]]),\n  tensor([[1.3959]]),\n  tensor([[-0.0172,  1.4947, -0.1980,  0.3552,  0.0185,  0.0304,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.6971130967140198,\n  tensor([[ 0.1726,  0.5585,  0.4565, -0.8544, -0.5156, -0.2410,  0.0000,  0.0000]]),\n  tensor([[-0.9603,  0.6626]]),\n  tensor([[-2.5699]]),\n  tensor([[ 0.1771,  0.5387,  0.4634, -0.8838, -0.5294, -0.2752,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.708808422088623,\n  tensor([[ 0.3682,  0.1119,  0.6927, -0.2071, -1.8716, -2.1575,  0.0000,  0.0000]]),\n  tensor([[-0.6272,  0.8322]]),\n  tensor([[-12.2123]]),\n  tensor([[ 0.3755,  0.1081,  0.6891, -0.2386, -1.9813, -2.1952,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-704.8673706054688,\n  tensor([[-0.0114,  1.4694, -0.1907,  0.4150,  0.0120,  0.0463,  0.0000,  0.0000]]),\n  tensor([[-0.7800, -0.7521]]),\n  tensor([[0.8698]]),\n  tensor([[-0.0133,  1.4782, -0.1965,  0.3887,  0.0155,  0.0697,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.028913818299770355,\n  tensor([[-0.0229,  1.5166, -0.1866,  0.3088,  0.0186, -0.0107,  0.0000,  0.0000]]),\n  tensor([[-0.2565, -0.4705]]),\n  tensor([[1.6701]]),\n  tensor([[-0.0248,  1.5230, -0.1866,  0.2821,  0.0181, -0.0107,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-0.19773167371749878,\n  tensor([[ 0.1817,  0.5183,  0.4634, -0.9105, -0.5432, -0.2752,  0.0000,  0.0000]]),\n  tensor([[-0.6041, -0.7947]]),\n  tensor([[-1.1140]]),\n  tensor([[ 0.1861,  0.4974,  0.4556, -0.9337, -0.5549, -0.2350,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (0.08580269664525986,\n  tensor([[-0.0286,  1.5345, -0.1922,  0.2551,  0.0156, -0.0380,  0.0000,  0.0000]]),\n  tensor([[-0.0289,  0.2930]]),\n  tensor([[1.7599]]),\n  tensor([[-0.0305,  1.5396, -0.1922,  0.2285,  0.0137, -0.0380,  0.0000,  0.0000]]),\n  tensor([[0.]])),\n (-705.0590209960938,\n  tensor([[-0.0101,  0.6114, -0.0379, -0.8326,  0.1151, -0.0312,  0.0000,  0.0000]]),\n  tensor([[-0.0447, -0.6087]]),\n  tensor([[-0.7848]]),\n  tensor([[-0.0105,  0.5920, -0.0431, -0.8599,  0.1146, -0.0102,  0.0000,  0.0000]]),\n  tensor([[0.]]))]"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "rbuff.minheap.heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}