{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit41770f51cb494085b126429b02db281f",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import myrl\n",
    "from myrl.environments import Envs\n",
    "from myrl.policies import GaussianPolicy, CategoricalPolicy\n",
    "from myrl.value_functions import ValueFunctionMLP, polyak\n",
    "from myrl.visualizer import showit\n",
    "from myrl.utils import clip_grad_norm_, ExperimentWriter, check_output, Discrete2Continuous\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "\n",
    "best = None\n",
    "bestr = float('-inf')\n",
    "bestep = -1\n",
    "wll = ExperimentWriter('tb/ppo_take_infinity__+++_tryagain_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "255 / 256"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(3,\n 1,\n GaussianPolicy(\n   (layers): ModuleList(\n     (0): Linear(in_features=3, out_features=32, bias=True)\n     (1): Linear(in_features=32, out_features=1, bias=True)\n     (2): Linear(in_features=32, out_features=1, bias=True)\n   )\n ))"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "idim = env.observation_space.shape[0]\n",
    "odim = env.action_space.shape[0]\n",
    "\n",
    "pi = GaussianPolicy([idim, 32, odim], std=0.15)\n",
    "# pi = CategoricalPolicy([idim, 32, 5])\n",
    "picon = Discrete2Continuous(-2, 2, 5, pi.act)\n",
    "vfunc = ValueFunctionMLP([idim, 128, 32, odim])\n",
    "tvfunc = copy.deepcopy(vfunc)\n",
    "envs = Envs('Pendulum-v0', 256)\n",
    "envs.evenout(200)\n",
    "idim, odim, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(pi.parameters(), lr=1e-2)\n",
    "copt = torch.optim.Adam(vfunc.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "15975952148438 r -7.180079460144043  td 2.0554332733154297 clip 0.37890625\n284 300.3746643066406 r -7.183871269226074  td 0.027320606634020805 clip 0.421875\n288 -959.1957397460938 r -7.254042148590088  td 0.0681377574801445 clip 0.33203125\n292 -383.1043701171875 r -7.201418399810791  td 0.07073328644037247 clip 0.09375\n296 -290.7246398925781 r -7.1575541496276855  td 0.024050330743193626 clip 0.2109375\n300 -471.5356140136719 r -6.99013614654541  td 0.07959326356649399 clip 0.1484375\n304 270.5162048339844 r -6.922449111938477  td 0.05915914848446846 clip 0.3828125\n308 -181.8395233154297 r -6.979433059692383  td 0.061166178435087204 clip 0.05859375\n312 -135.28298950195312 r -6.979071617126465  td 0.20902277529239655 clip 0.25390625\n316 -134.2525634765625 r -6.927289962768555  td 0.5249055624008179 clip 0.2421875\n320 -113.45204162597656 r -6.973128795623779  td 0.3565579056739807 clip 0.0390625\n324 -680.51171875 r -6.879942893981934  td 0.8860902786254883 clip 0.49609375\n328 -89.8789291381836 r -6.793881416320801  td 0.33375850319862366 clip 0.25\n332 -370.223388671875 r -6.795332908630371  td 0.05282954126596451 clip 0.1953125\n336 87.37295532226562 r -6.833456516265869  td 0.061264678835868835 clip 0.3125\n340 114.53423309326172 r -6.965831279754639  td 2.0441322326660156 clip 0.4296875\n344 -246.2969207763672 r -6.826117992401123  td 0.16246731579303741 clip 0.19140625\n348 -268.7547302246094 r -6.8298749923706055  td 0.18566037714481354 clip 0.08203125\n352 -410.2592468261719 r -7.043542385101318  td 0.9046748876571655 clip 0.453125\n356 203.99708557128906 r -7.16848087310791  td 1.995544672012329 clip 0.21484375\n360 -616.4884643554688 r -7.115950107574463  td 0.42836812138557434 clip 0.18359375\n198 /2000364 -56.33966064453125 r -7.070468902587891  td 0.07113534212112427 clip 0.20703125\n368 -756.2531127929688 r -6.950875282287598  td 0.6334114670753479 clip 0.3203125\n372 68.3625259399414 r -6.820001602172852  td 0.772250771522522 clip 0.34375\n376 -366.90411376953125 r -6.827822685241699  td 0.9946029186248779 clip 0.265625\n380 -72.10765075683594 r -6.972394943237305  td 0.1949559599161148 clip 0.26171875\n384 -349.3802490234375 r -6.885185241699219  td 0.21104709804058075 clip 0.25\n388 -368.95294189453125 r -7.029265403747559  td 0.1884622871875763 clip 0.125\n392 -185.25682067871094 r -7.03533935546875  td 0.2841774821281433 clip 0.13671875\n396 1061.8251953125 r -6.907357215881348  td 0.25851815938949585 clip 0.37890625\n400 50.831329345703125 r -6.916971206665039  td 1.2449644804000854 clip 0.1171875\n404 -92.52070617675781 r -6.856131076812744  td 0.5765606760978699 clip 0.4453125\n408 -237.7169647216797 r -6.759940147399902  td 0.09635674208402634 clip 0.09375\n412 -25.801546096801758 r -6.789852619171143  td 0.0920640230178833 clip 0.18359375\n416 -811.4397583007812 r -6.8277363777160645  td 1.1424087285995483 clip 0.25\n420 423.84844970703125 r -6.747477054595947  td 0.40631991624832153 clip 0.30859375\n424 -507.6799621582031 r -6.744115352630615  td 0.2886982262134552 clip 0.24609375\n428 -1102.743896484375 r -6.768269062042236  td 0.08363234251737595 clip 0.22265625\n432 127.93573760986328 r -6.6691789627075195  td 0.09048844873905182 clip 0.23828125\n436 -49.2822380065918 r -6.546416759490967  td 0.05896386504173279 clip 0.390625\n440 -873.2449340820312 r -6.563458442687988  td 1.2996772527694702 clip 0.20703125\n444 -28.59737777709961 r -6.710448265075684  td 1.2162903547286987 clip 0.4375\n448 132.51268005371094 r -6.867865562438965  td 0.19071678817272186 clip 0.16796875\n452 -32.62683868408203 r -6.750757217407227  td 0.573660135269165 clip 0.22265625\n456 69.94009399414062 r -6.598790645599365  td 0.054895319044589996 clip 0.30078125\n460 -306.4047546386719 r -6.644843101501465  td 0.06681801378726959 clip 0.11328125\n464 -392.94464111328125 r -6.702009677886963  td 0.11359182000160217 clip 0.13671875\n468 -14.674477577209473 r -6.70076847076416  td 0.6022319793701172 clip 0.140625\n472 -241.8250732421875 r -6.552725791931152  td 1.5958813428878784 clip 0.13671875\n476 -392.5549011230469 r -6.569625377655029  td 0.3406144678592682 clip 0.1953125\n480 -287.6596374511719 r -6.593211650848389  td 0.5215663909912109 clip 0.26171875\n198 /2000484 -183.27813720703125 r -6.709065914154053  td 4.078778266906738 clip 0.0390625\n488 -315.7148132324219 r -6.67965030670166  td 0.6038874387741089 clip 0.15234375\n492 -498.1443176269531 r -6.73027229309082  td 0.09987713396549225 clip 0.05859375\n496 -648.7749633789062 r -6.779788494110107  td 0.4900776147842407 clip 0.23828125\n500 -70.84719848632812 r -6.856594562530518  td 3.832914113998413 clip 0.33203125\n504 -144.01097106933594 r -6.841490268707275  td 3.3320395946502686 clip 0.16796875\n508 -586.1316528320312 r -6.616553783416748  td 0.5387906432151794 clip 0.1796875\n512 -197.3273162841797 r -6.673417091369629  td 0.40533769130706787 clip 0.37109375\n516 -707.9563598632812 r -6.660458564758301  td 0.22129173576831818 clip 0.17578125\n520 239.94175720214844 r -6.8717265129089355  td 0.20481806993484497 clip 0.1640625\n524 -276.1466369628906 r -6.85886812210083  td 0.15310414135456085 clip 0.33203125\n528 -306.48150634765625 r -6.5948872566223145  td 0.106309674680233 clip 0.125\n532 -448.010498046875 r -6.440659999847412  td 0.10345041751861572 clip 0.265625\n536 -842.4915161132812 r -6.437342643737793  td 0.1071164682507515 clip 0.06640625\n540 -704.2620239257812 r -6.3604736328125  td 0.08561830222606659 clip 0.359375\n544 -593.4337158203125 r -6.356225490570068  td 0.12416280061006546 clip 0.15234375\n548 -333.2540283203125 r -6.543321132659912  td 0.12442082166671753 clip 0.09375\n552 -229.69833374023438 r -6.71699333190918  td 0.23191380500793457 clip 0.23046875\n556 -617.3905029296875 r -6.701136112213135  td 0.08467330783605576 clip 0.16796875\n560 -500.0986022949219 r -6.624007225036621  td 0.09928567707538605 clip 0.21484375\n564 128.84130859375 r -6.497674465179443  td 0.0737803503870964 clip 0.0546875\n568 128.63409423828125 r -6.450294494628906  td 0.1333935409784317 clip 0.5390625\n572 -904.2259521484375 r -6.553590774536133  td 0.19636383652687073 clip 0.28125\n576 -174.0224609375 r -6.6322550773620605  td 3.2907543182373047 clip 0.21875\n580 -507.40911865234375 r -6.689663887023926  td 0.9045621752738953 clip 0.4140625\n584 -159.46771240234375 r -6.665908336639404  td 0.3770371377468109 clip 0.12109375\n588 -308.0105285644531 r -6.413872241973877  td 0.16186828911304474 clip 0.1953125\n592 -170.88949584960938 r -6.4075703620910645  td 0.1838790327310562 clip 0.2421875\n596 132.56613159179688 r -6.3925676345825195  td 0.5634307265281677 clip 0.09375\n600 -640.8663330078125 r -6.17527437210083  td 2.855586051940918 clip 0.22265625\n198 /2000604 -922.9810791015625 r -6.297106742858887  td 1.3853533267974854 clip 0.2734375\n608 -217.58377075195312 r -6.53632926940918  td 0.4443652033805847 clip 0.1875\n612 -93.91801452636719 r -6.530154228210449  td 0.20834125578403473 clip 0.05078125\n616 -398.991455078125 r -6.4009175300598145  td 0.13225732743740082 clip 0.171875\n620 -696.1423950195312 r -6.265901565551758  td 0.18253497779369354 clip 0.046875\n624 -275.3758850097656 r -6.235430717468262  td 1.3664990663528442 clip 0.109375\n628 -57.78418731689453 r -6.293825149536133  td 0.21062979102134705 clip 0.109375\n632 -420.9187927246094 r -6.152238845825195  td 0.14447538554668427 clip 0.08203125\n636 233.15623474121094 r -5.908139705657959  td 0.1948881894350052 clip 0.28125\n640 -449.479736328125 r -5.834630966186523  td 1.8667646646499634 clip 0.12109375\n644 -394.79742431640625 r -5.942461013793945  td 1.6357730627059937 clip 0.26953125\n648 -352.7098388671875 r -6.114213466644287  td 0.7762985825538635 clip 0.234375\n652 -121.24574279785156 r -6.12509822845459  td 0.5815620422363281 clip 0.12109375\n656 -571.838134765625 r -5.9061279296875  td 0.17499098181724548 clip 0.1875\n660 -497.3594055175781 r -5.884148120880127  td 0.16367454826831818 clip 0.16015625\n664 -119.04388427734375 r -5.769662380218506  td 0.16110095381736755 clip 0.36328125\n668 -130.28224182128906 r -5.654382705688477  td 0.12343963980674744 clip 0.26171875\n672 511.0692138671875 r -5.582303047180176  td 0.27097734808921814 clip 0.2265625\n676 -15.275552749633789 r -5.625386714935303  td 0.13011719286441803 clip 0.2265625\n680 -519.7942504882812 r -5.419013977050781  td 0.2697673439979553 clip 0.2109375\n684 458.675537109375 r -5.225736618041992  td 5.0739874839782715 clip 0.26171875\n688 -615.7594604492188 r -5.230209827423096  td 1.136640191078186 clip 0.2265625\n692 -261.4280090332031 r -5.466571807861328  td 3.2435240745544434 clip 0.18359375\n696 -900.8331909179688 r -5.826506614685059  td 0.21558400988578796 clip 0.0859375\n700 -305.0910339355469 r -6.024327278137207  td 3.624206066131592 clip 0.1171875\n704 -499.3773193359375 r -5.942733287811279  td 0.6821621656417847 clip 0.15625\n708 -391.60595703125 r -5.760868072509766  td 0.2656756341457367 clip 0.078125\n712 -619.4088745117188 r -5.419360160827637  td 0.25295886397361755 clip 0.1015625\n716 161.59693908691406 r -4.920760631561279  td 0.24976739287376404 clip 0.12109375\n720 24.231895446777344 r -4.993009567260742  td 0.15223060548305511 clip 0.2890625\n198 /2000724 18.13936996459961 r -4.990298748016357  td 0.19198179244995117 clip 0.15625\n728 -495.141357421875 r -4.858511924743652  td 0.17624127864837646 clip 0.2109375\n732 -1192.3563232421875 r -4.978606224060059  td 0.17331919074058533 clip 0.25\n736 -402.1910095214844 r -5.358620643615723  td 0.2505965232849121 clip 0.1328125\n740 -442.5850830078125 r -5.745800018310547  td 0.1756540834903717 clip 0.3203125\n744 -680.836181640625 r -5.438760280609131  td 0.26740163564682007 clip 0.05859375\n748 -62.38454818725586 r -5.34854793548584  td 0.2679762542247772 clip 0.2421875\n752 -466.6763000488281 r -5.164273738861084  td 0.19797205924987793 clip 0.04296875\n756 -551.6705932617188 r -5.205962181091309  td 0.5553456544876099 clip 0.24609375\n760 -558.77294921875 r -5.16482400894165  td 0.7833988666534424 clip 0.00390625\n764 -535.612548828125 r -5.130955219268799  td 1.7530295848846436 clip 0.16796875\n768 -417.1166687011719 r -5.289960861206055  td 0.6216466426849365 clip 0.16796875\n772 -311.8900451660156 r -5.520077228546143  td 0.30975109338760376 clip 0.15234375\n776 -196.76089477539062 r -5.524876117706299  td 0.33878499269485474 clip 0.1328125\n780 -15.384703636169434 r -5.33640718460083  td 0.3202824294567108 clip 0.09375\n784 -415.5986022949219 r -5.22340726852417  td 4.908880710601807 clip 0.05078125\n788 -671.9755859375 r -5.225002765655518  td 4.62169075012207 clip 0.078125\n792 -243.50582885742188 r -5.601095676422119  td 0.626497209072113 clip 0.234375\n796 477.9718933105469 r -5.600277900695801  td 0.30213287472724915 clip 0.21484375\n800 -932.0752563476562 r -5.365612030029297  td 0.16224579513072968 clip 0.15625\n804 -191.34474182128906 r -5.195705413818359  td 0.2932209372520447 clip 0.19921875\n808 -869.1341552734375 r -5.256488800048828  td 0.16020594537258148 clip 0.046875\n812 -446.3485107421875 r -5.379971981048584  td 0.20592981576919556 clip 0.12109375\n816 167.87908935546875 r -5.149155616760254  td 0.21618884801864624 clip 0.33203125\n820 -18.229801177978516 r -5.070423603057861  td 0.21963997185230255 clip 0.09375\n824 -129.1140899658203 r -5.007407188415527  td 0.19792282581329346 clip 0.1171875\n828 -258.337158203125 r -5.075862884521484  td 0.30300310254096985 clip 0.08984375\n832 -943.6939086914062 r -5.081257343292236  td 0.24918997287750244 clip 0.20703125\n836 -731.3877563476562 r -4.9533185958862305  td 0.2557278275489807 clip 0.0703125\n840 -292.41851806640625 r -4.86271333694458  td 0.2695736289024353 clip 0.21484375\n198 /2000844 -1200.7098388671875 r -4.760874271392822  td 0.3468843102455139 clip 0.34375\n848 561.8062133789062 r -4.58988094329834  td 1.784171462059021 clip 0.16015625\n852 -8.091936111450195 r -4.568676948547363  td 0.39923569560050964 clip 0.26171875\n856 -533.3759155273438 r -4.653603553771973  td 0.2431996762752533 clip 0.15234375\n860 -18.930341720581055 r -4.758877754211426  td 0.35466858744621277 clip 0.08203125\n864 -870.0911254882812 r -4.638932228088379  td 0.27320465445518494 clip 0.109375\n868 -606.9581298828125 r -4.64937686920166  td 0.18943092226982117 clip 0.21875\n872 -259.4335021972656 r -4.593043804168701  td 0.26137256622314453 clip 0.1328125\n876 -320.90301513671875 r -4.590676307678223  td 5.484099864959717 clip 0.26171875\n880 -1042.248046875 r -4.627073764801025  td 0.3515675365924835 clip 0.22265625\n884 -512.8357543945312 r -4.344602108001709  td 0.18334288895130157 clip 0.10546875\n888 -215.43763732910156 r -4.447795867919922  td 0.13819095492362976 clip 0.05859375\n892 331.82391357421875 r -4.57944917678833  td 0.9522172808647156 clip 0.3515625\n896 -258.1532897949219 r -4.774590492248535  td 0.3331652879714966 clip 0.140625\n900 -539.5974731445312 r -4.6921210289001465  td 0.3609960377216339 clip 0.1953125\n904 -555.2638549804688 r -4.696902275085449  td 0.3126716613769531 clip 0.3359375\n908 -875.435302734375 r -4.568297386169434  td 1.4027538299560547 clip 0.109375\n912 -111.55530548095703 r -4.384059429168701  td 0.43028098344802856 clip 0.03125\n916 -595.4057006835938 r -4.472733497619629  td 5.373290538787842 clip 0.1171875\n920 -554.639404296875 r -4.630649566650391  td 0.6765331625938416 clip 0.09765625\n924 -666.3048706054688 r -4.649212837219238  td 2.0892860889434814 clip 0.1953125\n928 -950.8975830078125 r -4.407398700714111  td 2.5826029777526855 clip 0.1875\n932 -810.1258544921875 r -4.136225700378418  td 0.5643462538719177 clip 0.1484375\n936 -909.3396606445312 r -4.00809383392334  td 0.7440802454948425 clip 0.17578125\n940 -173.7013397216797 r -4.0577545166015625  td 0.6106760501861572 clip 0.1484375\n944 -264.07080078125 r -4.085746765136719  td 0.7165696620941162 clip 0.0\n948 -737.8084716796875 r -3.791252851486206  td 1.6231663227081299 clip 0.1171875\n952 -1133.6944580078125 r -3.661518096923828  td 2.0385985374450684 clip 0.125\n956 -900.5182495117188 r -3.3812785148620605  td 3.7343804836273193 clip 0.3125\n960 -1345.415283203125 r -3.3154916763305664  td 2.3771395683288574 clip 0.140625\n198 /2000964 667.0028076171875 r -3.2661194801330566  td 1.6081459522247314 clip 0.09765625\n968 -346.9060974121094 r -3.1667323112487793  td 1.7448735237121582 clip 0.03125\n972 -826.3505859375 r -3.042555332183838  td 10.751211166381836 clip 0.08203125\n976 -617.405029296875 r -3.1918063163757324  td 3.596731662750244 clip 0.16796875\n980 -682.3163452148438 r -3.3542492389678955  td 0.8764909505844116 clip 0.31640625\n984 -185.7068328857422 r -3.272604465484619  td 0.7690601944923401 clip 0.12890625\n988 -639.662353515625 r -2.989095687866211  td 0.4953502118587494 clip 0.12109375\n992 -567.4764404296875 r -2.5826046466827393  td 0.5180181860923767 clip 0.046875\n996 -937.7510986328125 r -2.567382335662842  td 0.8362926840782166 clip 0.33984375\n1000 -593.2979125976562 r -2.635712146759033  td 0.780307412147522 clip 0.05078125\n1004 90.93173217773438 r -2.7123186588287354  td 0.4925248622894287 clip 0.05078125\n1008 -533.71240234375 r -2.5893683433532715  td 0.6941393613815308 clip 0.32421875\n1012 -374.514892578125 r -2.360534906387329  td 0.9019718766212463 clip 0.06640625\n1016 -1498.1318359375 r -2.4608278274536133  td 0.8040056228637695 clip 0.25390625\n1020 -1058.1732177734375 r -2.5878231525421143  td 0.768120527267456 clip 0.109375\n1024 -1170.3072509765625 r -2.4820828437805176  td 0.474681556224823 clip 0.24609375\n1028 -1042.13916015625 r -2.579730749130249  td 0.6558713912963867 clip 0.11328125\n1032 131.47573852539062 r -2.5536949634552  td 0.40662580728530884 clip 0.14453125\n1036 -313.063720703125 r -2.5197834968566895  td 0.518808126449585 clip 0.08984375\n1040 -210.8633575439453 r -2.700218439102173  td 0.7304068207740784 clip 0.171875\n1044 -399.4729309082031 r -2.510477304458618  td 0.6115479469299316 clip 0.06640625\n1048 -962.5928344726562 r -2.1836278438568115  td 0.4676091969013214 clip 0.05078125\n1052 -681.1350708007812 r -2.0439984798431396  td 0.564794659614563 clip 0.20703125\n1056 -902.227294921875 r -1.9636328220367432  td 0.9021159410476685 clip 0.046875\n1060 -680.0576782226562 r -1.9251947402954102  td 1.096825122833252 clip 0.0859375\n1064 -599.6742553710938 r -2.1518359184265137  td 1.034366488456726 clip 0.12109375\n1068 -1899.6136474609375 r -2.0838847160339355  td 0.8450624346733093 clip 0.22265625\n1072 -283.1630859375 r -1.8435826301574707  td 0.7446880340576172 clip 0.109375\n1076 -820.7844848632812 r -1.6745707988739014  td 0.5472439527511597 clip 0.078125\n1080 -747.9423828125 r -1.4406667947769165  td 0.5084459185600281 clip 0.2734375\n198 /20001084 -968.9279174804688 r -1.5413589477539062  td 0.8299112319946289 clip 0.12890625\n1088 -738.1377563476562 r -1.6546839475631714  td 0.47149190306663513 clip 0.05078125\n1092 -633.27392578125 r -1.6105034351348877  td 1.5965385437011719 clip 0.09375\n1096 -911.626220703125 r -1.8566555976867676  td 1.3379435539245605 clip 0.09375\n1100 -828.6069946289062 r -1.7496472597122192  td 1.0010093450546265 clip 0.1171875\n1104 -224.38941955566406 r -1.3694415092468262  td 0.6061283946037292 clip 0.4140625\n1108 -697.9782104492188 r -1.176826000213623  td 0.8739151358604431 clip 0.2109375\n1112 -164.6552276611328 r -1.1831835508346558  td 1.1976161003112793 clip 0.046875\n1116 -1410.75048828125 r -1.2761585712432861  td 0.9528917670249939 clip 0.078125\n1120 -598.5162963867188 r -1.1938849687576294  td 0.9589545726776123 clip 0.46484375\n1124 -717.29833984375 r -1.3362553119659424  td 1.628811240196228 clip 0.1875\n1128 -1510.7374267578125 r -1.5689105987548828  td 0.9109123945236206 clip 0.10546875\n1132 -747.9658203125 r -1.4215410947799683  td 1.293313980102539 clip 0.0546875\n1136 -1499.24658203125 r -1.3303755521774292  td 1.0293151140213013 clip 0.46484375\n1140 -139.35836791992188 r -1.096805453300476  td 0.7039920091629028 clip 0.109375\n1144 -1060.3333740234375 r -1.2090522050857544  td 0.7433075904846191 clip 0.08203125\n1148 -13.256072998046875 r -1.2812411785125732  td 0.7828367948532104 clip 0.37890625\n1152 5.345791816711426 r -1.1585547924041748  td 1.1653003692626953 clip 0.37109375\n1156 -411.4093322753906 r -0.9982212781906128  td 0.470436692237854 clip 0.38671875\n1160 -387.95550537109375 r -1.0507750511169434  td 1.4006800651550293 clip 0.359375\n1164 -428.88885498046875 r -1.093577265739441  td 2.006783962249756 clip 0.0546875\n1168 -1089.8450927734375 r -1.2686431407928467  td 1.856062889099121 clip 0.1796875\n1172 -193.94923400878906 r -1.276247501373291  td 2.072089910507202 clip 0.04296875\n1176 -4.872493267059326 r -1.3753445148468018  td 0.950655460357666 clip 0.06640625\n1180 -1293.5050048828125 r -1.3175573348999023  td 0.6982085704803467 clip 0.05078125\n1184 -1550.242919921875 r -1.2981187105178833  td 1.4217352867126465 clip 0.109375\n1188 -27.710105895996094 r -1.372115969657898  td 1.449879765510559 clip 0.15625\n1192 228.12522888183594 r -1.2531006336212158  td 0.9809794425964355 clip 0.3671875\n1196 -818.8851318359375 r -0.9655908942222595  td 0.6968319416046143 clip 0.05078125\n1200 -855.3872680664062 r -0.6874732375144958  td 0.6926801800727844 clip 0.20703125\n198 /20001204 -903.2459106445312 r -0.7142165899276733  td 2.8206138610839844 clip 0.18359375\n1208 -574.7627563476562 r -0.6985316872596741  td 1.2462222576141357 clip 0.63671875\n1212 -988.9478759765625 r -0.8659207224845886  td 4.761177062988281 clip 0.10546875\n1216 -723.8368530273438 r -0.9894994497299194  td 2.0841422080993652 clip 0.05078125\n1220 -931.346435546875 r -0.9466400146484375  td 1.000989317893982 clip 0.109375\n1224 -836.9520263671875 r -0.9414913654327393  td 1.427396535873413 clip 0.09375\n1228 -6.137182712554932 r -1.184194564819336  td 1.3700854778289795 clip 0.1015625\n1232 -534.005859375 r -1.3425629138946533  td 0.9651082754135132 clip 0.08984375\n1236 -442.0860595703125 r -1.355175495147705  td 4.958999156951904 clip 0.375\n1240 -264.691650390625 r -1.4298423528671265  td 2.649662733078003 clip 0.3515625\n1244 -255.76576232910156 r -1.513700246810913  td 1.5021096467971802 clip 0.1171875\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-02453891038f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtvfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moldobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mcopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wll.new()\n",
    "writer = wll.writer\n",
    "eps = 10000\n",
    "gamma = 0.97\n",
    "\n",
    "best = None\n",
    "bestr = float('-inf')\n",
    "bestep = -1\n",
    "\n",
    "for ep in range(eps):\n",
    "    oldobs, a, r, obs, d, oldprobs, smpls, _ = envs.rollout(pi.act, length=1, debug=0)\n",
    "    adv = r - vfunc(oldobs).detach() + vfunc(obs).detach()*gamma*(1-d)\n",
    "    adv = ((adv-adv.mean())/(adv.std()+1e-8)).detach()\n",
    "    oldobs, a, r, obs, d, oldprobs, smpls = oldobs.detach(), a.detach(), r.detach(), obs.detach(), d.detach(), oldprobs.detach(), smpls.detach()\n",
    "    for optstep in range(4):\n",
    "        _, (newprobs, _, ps) = pi.act(oldobs, smpl=smpls)\n",
    "        ratio = torch.exp(newprobs-oldprobs)\n",
    "        e = 0.15\n",
    "        clipped = (ratio != torch.clamp(ratio, 1-e, 1+e)).float().mean().detach().item()\n",
    "        loss = -torch.min(ratio*adv, torch.clamp(ratio, 1-e, 1+e)*adv).mean()\n",
    "        loss *= 100000\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(opt, 0.5)\n",
    "        opt.step()\n",
    "    \n",
    "    for optstep in range(4):\n",
    "        td = ((r + gamma*tvfunc(obs).detach() - vfunc(oldobs))**2).mean()\n",
    "        copt.zero_grad()\n",
    "        td.backward()\n",
    "        copt.step()\n",
    "\n",
    "    polyak(vfunc, tvfunc, 1-1/50)\n",
    "\n",
    "\n",
    "    if ep%4==0:\n",
    "        print(ep, loss.item(), \"r\", r.mean().item(), \" td\", td.item(), \"clip\", clipped)\n",
    "    if ep%120==0:\n",
    "        showit(env, pi.act)\n",
    "        env.close()\n",
    "    writer.add_scalar(\"a/loss\", loss.item(), ep)\n",
    "    writer.add_scalar(\"a/td\", td.item(), ep)\n",
    "    writer.add_scalar(\"a/reward\", r.mean(), ep)\n",
    "    writer.add_scalar(\"a/clipped\", clipped, ep)\n",
    "    writer.add_scalar(\"a/lr\", opt.param_groups[0]['lr'], ep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1.0115e-02, 9.9966e-02, 1.0002e+00],\n        [1.0000e+01, 1.0000e+02, 1.0000e+03]])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "m = torch.tensor([[0.01, 0.1, 1], [10, 100, 1000]])\n",
    "std = torch.zeros_like(m) + 0.0001 + 1 -1\n",
    "dis = torch.distributions.Normal(m, std)\n",
    "smpls = dis.sample()\n",
    "smpls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[2059.3035, 3760.7412,  877.5601],\n        [1357.1128, 1720.5764, 1893.9595]])"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "torch.exp(dis.log_prob(smpls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[7.6301, 8.2324, 6.7771],\n        [7.2131, 7.4504, 7.5464]])"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "dis.log_prob(smpls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if bestr < r.mean().item():\n",
    "        bestr = r.mean().item()\n",
    "        bestep = ep\n",
    "        best = copy.deepcopy(pi)\n",
    "    if ep - bestep > 25 and 0:\n",
    "        pi = copy.deepcopy(best)\n",
    "        opt = torch.optim.Adam(pi.parameters(), lr=1e-1)\n",
    "        shedualer_increase = torch.optim.lr_scheduler.MultiplicativeLR(opt, lambda ep:1.05)\n",
    "        shedualer_decrease = torch.optim.lr_scheduler.MultiplicativeLR(opt, lambda ep:0.7)\n",
    "        bestep = ep\n",
    "        print(\"swap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fa22e76effda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmyrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_batch_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "from myrl.utils import get_batch_obs\n",
    "bobs = get_batch_obs(env, 32)\n",
    "a, (d, s, h) = pi(bobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.5899]], grad_fn=<AddmmBackward>)"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "check_output(env, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myrl.policies import CategoricalPolicy\n",
    "pi2 = CategoricalPolicy([3, 16, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "198 /2000"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-2.1498235748277876"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "showit(env, pi.act)"
   ]
  }
 ]
}